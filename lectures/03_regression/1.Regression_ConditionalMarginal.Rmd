---
title: "Conditional and Marginal Effects in a Regression Context"
author: "Ashley I Naimi"
date: "Spring 2024"
urlcolor: blue
bibliography: ref.bib
link-citations: yes
output: 
    bookdown::pdf_book:
      base_format: tint::tintPdf
      toc: true
      number_sections: true
      includes:
        in_header: "../misc/preamble.tex"
      latex_engine: xelatex
    html_document:
      theme: readable
      toc: true
      toc_float: true
      number_sections: true
      css: "../misc/style.css"
---

```{r setup, include=FALSE}
library(knitr)
opts_chunk$set(tidy.opts=list(width.cutoff=40),tidy=TRUE)

packages <- c( "data.table","tidyverse","ggplot2","ggExtra","formatR",
               "gridExtra","skimr","here","Hmisc","RColorBrewer")

for (package in packages) {
  if (!require(package, character.only=T, quietly=T)) {
    install.packages(package, repos='http://lib.stat.cmu.edu/R/CRAN',dependencies=T)
  }
}

for (package in packages) {
  library(package, character.only=T)
}

remotes::install_github("rstudio/fontawesome")

library(fontawesome)

thm <- theme_classic() +
  theme(
    legend.position = "top",
    legend.background = element_rect(fill = "transparent", colour = NA),
    legend.key = element_rect(fill = "transparent", colour = NA)
  )
theme_set(thm)
```

\newpage
\onehalfspacing

\newpage
\onehalfspacing

# Introduction

Let's say we're interested in estimating the average treatment effect of some binary exposure $X$ on some outcome $Y$. We can estimate this effect on the difference scale:

$$\psi = E(Y^1 - Y^0)$$

On the ratio scale:

$$\psi = E(Y^1)/E(Y^0)$$

And, if the outcome is binary (assuming $Y \in [0,1]$), on the odds ratio scale:

$$\psi = \frac{E(Y^1)}{1 - E(Y^1)} \bigg / \frac{E(Y^0)}{1 - E(Y^0)}$$

Note that these are **average** or **marginal** effects, because they represent contrasts of averages of potential outcomes, with the averages taken over all individuals in the population of interest. This are different from **conditional** average treatment effects, or CATEs. In practice, we'd often use regression models to estimate the risk difference, risk ratio, and odds ratio. However, before we discuss regression modeling, it's important to understand some numerical properties of the risk difference, risk ratio, and odds ratio as a measure of association or effect.

:::{.rmdnote data-latex="{caution}"}

__Cautionary Note__: Communicating Risk Contrasts

|               Several people use different nomenclature to communicate risk differences, risk ratios, and odds ratios. Among these include "attributable risk", "excess risk", or "absolute risk increase/reduction" for the risk difference, and "relative risk" for the risk ratio or odds ratio. However, ambiguous terms such as "relative risk" should be avoided. The risk difference, risk ratio, and odds ratio are all measures of "relative risk." and thus this term should not be generally used. Additionally, as English language constructs, "attributable risk" and "excess risk" are both applicable to the risk difference and risk ratio. To avoid confusion, it's best to use the relevantly accurate terminology: i.e., risk difference, risk ratio, or odds ratio.

:::

# Collapsibility versus Noncollapsibility of Association Contrasts

Consider the following data adapted from @Greenland2005b:

```{r, include = T, eval = T}
# table 1 from greenland
d <- data.frame(
  z = c(1,1,1,1,0,0,0,0),
  x = c(1,1,0,0,1,1,0,0),
  y = c(1,0,1,0,1,0,1,0),
  n = c(200,50,150,100,100,150,50,200)
)

d <- d %>%
  uncount(n) 
```

These data lead to the following contingency table :

```{r greenlandtable, out.width="15cm", fig.align='center', fig.fullwidth = TRUE, fig.cap="Table 1 from Greenland 2005 showing examples of collapsibility and noncollapsibility in a three-way distribution.", echo=F}
knitr::include_graphics(here("_images", "Greenland2005Fig.pdf"))
```

This Table shows some important properties of the risk difference, risk ratio, and odds ratio. First, let's ensure we understand it's elements. 

The contingency tables in Table 1 show the proportions of the outcome $Y$ stratified by $Z$ and overall (Marginal) for each level of $X$. Importantly, the data in this Table was generated from a model where $Z$ did not affect the exposure $X$. The only causal relations are between the exposure $X$ and the outcome $Y$, and the covariate $Z$ and the outcome $Y$.

These proportions are used to then compute the "risks" or the $P(Y = 1)$. From these probabilities, we can compute risk differences, risk ratios, and odds ratios.^[Be sure you know how to compute these.]

Notice how these results differ between the $Z$ strata and overall:

- The risk difference is 0.2 across stratum and overall. This is true even though the absolute risks differ across strata and overall. This is a general phenomenon associated with the risk difference. That is, across strata of *a variable that is not causally related to or associated with the exposure* the risk difference is constant and equal to the overall risk difference. For this reason, we say that the **risk difference is strictly collapsible.**

- The risk ratio is 1.33 for the $Z = 1$ stratum and 2.00 for $Z = 0$. Furthermore, overall, the risk ratio is 1.5. While these three risk ratios are different, they are related in an important way. Note that the weighted average of the stratum specific risk ratios equal the overall risk ratio:

\begin{align*}
& \frac{P(Z = 1)P(Y = 1 \mid X = 1, Z = 1) + P(Z = 0)P(Y = 1 \mid X = 1, Z = 0)}{P(Z = 1)P(Y = 1 \mid X = 0, Z = 1) + P(Z = 0)P(Y = 1 \mid X = 0, Z = 0)} \\
& = \frac{0.5(0.80) + 0.5(0.40)}{0.5(0.60) + 0.5(0.20)} \\
& = 1.5
\end{align*}

Thus, the information that is contained in the overall risk ratio is available in the stratum specific risk ratios, but must be transformed appropriately. For this reason, we say that the **risk ratio is collapsible.**

- The odds ratio is 2.67 in both strata created by $Z$. However, the overall odds ratio is less than the stratum specific odds ratios, with a value of 2.25. Thus, there is no weighted combination of the stratum specific estimates that yield the overall estimate. Conceptually, it is almost as though the stratum specific odds ratios are capturing something completely different from the overall odds ratio. For this reason, we say that the **odds ratio is noncollapsible.** Note also the direction: the stratum specific odds ratio will always be greater than or equalled to the overall or marginally adjusted odds ratio.


# Noncollapsibility in a Regression Context

Noncollapsibility is one reason we have to be aware of the differences between unadjusted, conditionally adjusted, and marginally adjusted regression models. More specifically, the issue is about understanding the differences between conditionally and marginally adjusted effects.

## Unadjusted Regression Models

An unadjusted regression model yielding an unadjusted effect can be implemented as:

```{r}

mod_unadjusted <- glm(y ~ x, data = d, family = binomial("logit"))

```

We can compute the effect of interest by taking the exponent of the coefficient for the exposure from this model:

```{r}
exp(summary(mod_unadjusted)$coefficients[2,1])
```

We can also compute the marginal effect by taking the average of the predictions from this model under exposed and unexposed settings and constructing an odds ratio with them:

```{r}

mu1 <- mean(predict(mod_unadjusted, newdata = transform(d, x = 1), type = "response"))
mu0 <- mean(predict(mod_unadjusted, newdata = transform(d, x = 0), type = "response"))


marg_OR <- (mu1/(1-mu1))/(mu0/(1-mu0))

marg_OR

```

Note that, in this case where there are no other variables in the model, the conditional and marginal odds ratio are equivalent.

## Conditionally Adjusted Model

We can also adjust for $Z$ in two ways. The first is in a conditionally adjusted model. To do this, we simply add $Z$ to the unadjusted model, and then read the coefficient for the exposure from the model:

```{r}

mod_adjusted <- glm(y ~ x + z, data = d, family = binomial("logit"))

exp(summary(mod_adjusted)$coefficients[2,1])

```

Note that in this conditionally adjusted regression model, the odds ratio is `r round(exp(summary(mod_adjusted)$coefficients[2,1]), 2)`, which is larger than the unadjusted coefficient. This is true even though $Z$ is not a confounder, collider, or any other variable of causal importance. That is, simply adjusting for a variable representing "noise" in the system yields a conditionally adjusted odds ratio that is larger than the unadjusted coefficient. 

This is noncollapsibility of the odds ratio. 

## Marginally Adjusted Model

What happens if we adjust our model for $Z$, and then deploy the marginally adjusted approach to compute the odds ratio?:

```{r}

mu1 <- mean(predict(mod_adjusted, newdata = transform(d, x = 1), type = "response"))
mu0 <- mean(predict(mod_adjusted, newdata = transform(d, x = 0), type = "response"))


marg_OR <- (mu1/(1-mu1))/(mu0/(1-mu0))

marg_OR

```

We can see that, even though our regression model includes the noise variable $Z$, the marginally adjusted odds ratio is once again `r round(marg_OR, 2)`.

<!-- ## Interpretations of Conditionally and Marginally Adjusted Effects -->

<!-- This feature of the odds ratio (noncollapsibility) has long lead to confusion over how to interpret conditionally versus marginally adjusted effects.  -->

# Conditionally Adjusted Parametric Regression: An NHEFS Example

Let's see how some of these concepts can be important in a more realistic setting. Suppose we wanted to use the NHEFS data to estimate the confounder adjusted effect of quitting smoking on weight change. Suppose further that we were interested specifically in whether an individual's weight change was greater than the median value in the data. 

```{r, message=F, warning=F}

pacman::p_load(broom, tidyverse, boot)

nhefs <- read_csv(here("data","nhefs.csv")) %>% 
  mutate(wt_delta = as.numeric(wt82_71>median(wt82_71)))

#' Quick view of data
dim(nhefs)

names(nhefs)
```

A typical approach would be to use a generalized linear model to regress the indicator of weight change against the indicator of whether the individual quit smoking:

```{r, warning = F, message = F}
#' Here, we start fitting relevant regression models to the data.
#' modelForm is a regression argument that one can use to regress the 
#' outcome (wt_delta) against the exposure (qsmk) and selected confounders.

formulaVars <- "qsmk + sex + age + income + sbp + dbp + price71 + tax71 + race"
modelForm <- as.formula(paste0("wt_delta ~", formulaVars))
modelForm

#' This model can be used to quantify a conditionally adjusted 
#' odds ratio with correct standard error
modelOR <- glm(modelForm,data=nhefs,family = binomial("logit"))

summary(modelOR)

tidy(modelOR)[2,]

exp(tidy(modelOR)[2,2])

```

If we were interested in estimating conditionally adjusted risk differences or risk ratios for the effect of quitting smoking, we could use a similar approach with the identity link function, ordinary least squares, or Poisson regression [@Zou2004,@Naimi2020]:

```{r}

#' This model can be used to quantify a conditionally adjusted 
#' risk ratio with INCORRECT standard error
modelRR <- glm(modelForm, data=nhefs, family = poisson("log"))

summary(modelRR)

tidy(modelRR)[2,]

exp(tidy(modelRR)[2,2])

#' This model can be used to quantify a conditionally adjusted 
#' risk difference with INCORRECT standard error
modelRD <- lm(modelForm, data=nhefs)

summary(modelRD)

tidy(modelRD)[2,]

```


# Marginally Adjusted Parametric Regression: An NHEFS Example

Another approach to obtaining risk differences, risk ratios, and odds ratios from GLMs that are not subject to the limitations noted above is to use marginal standardization, which is equivalent to g computation (aka the parametric g formula) when the exposure is measured at a single time point [@Naimi2016b]. This process gives us marginally adjusted effects, and can be implemented by fitting a single logistic model, regressing the binary outcome against all confounder variables. But instead of reading the coefficients the model, one can obtain odds ratios, risk ratios, or risk differences by using this model to generate predicted risks for each individual under “exposed” and “unexposed” scenarios in the dataset. To obtain standard errors, the entire procedure must be bootstrapped.^[Though some forthcoming work (not by me) has developed an analytic solution to the variance estimation problem in a marginally standardized regression framework.]

Here is some code to implement this marginal standardization in the NHEFS data:

```{r, warning = F, message = F}
#' Regress the outcome against the confounders with interaction
ms_model <- glm(modelForm,data=nhefs,family=binomial("logit"))
##' Generate predictions for everyone in the sample to obtain 
##' unexposed (mu0 predictions) and exposed (mu1 predictions) risks.
mu1 <- predict(ms_model,newdata=transform(nhefs,qsmk=1),type="response")
mu0 <- predict(ms_model,newdata=transform(nhefs,qsmk=0),type="response")

#' Marginally adjusted odds ratio
marg_stand_OR <- (mean(mu1)/mean(1-mu1))/(mean(mu0)/mean(1-mu0))
#' Marginally adjusted risk ratio
marg_stand_RR <- mean(mu1)/mean(mu0)
#' Marginally adjusted risk difference
marg_stand_RD <- mean(mu1)-mean(mu0)

#' Using the bootstrap to obtain confidence intervals for the marginally adjusted 
#' risk ratio and risk difference.
bootfunc <- function(data,index){
  boot_dat <- data[index,]
  ms_model <- glm(modelForm,data=boot_dat,family=binomial("logit"))
  mu1 <- predict(ms_model,newdata=transform(boot_dat,qsmk=1),type="response")
  mu0 <- predict(ms_model,newdata=transform(boot_dat,qsmk=0),type="response")
  
  marg_stand_OR_ <- (mean(mu1)/mean(1-mu1))/(mean(mu0)/mean(1-mu0))
  marg_stand_RR_ <- mean(mu1)/mean(mu0)
  marg_stand_RD_ <- mean(mu1)-mean(mu0)
  res <- c(marg_stand_RD_,marg_stand_RR_,marg_stand_OR_)
  return(res)
}

#' Run the boot function. Set a seed to obtain reproducibility
set.seed(123)
boot_res <- boot(nhefs,bootfunc,R=2000)

boot_RD <- boot.ci(boot_res,index=1)
boot_RR <- boot.ci(boot_res,index=2)
boot_OR <- boot.ci(boot_res,index=3)

marg_stand_OR
marg_stand_RR
marg_stand_RD

boot_RD
boot_RR
boot_OR

```

This marginal standardization approach yields an estimate of the average treatment effect under the required identifiability assumptions. However, it assumes a constant effect of qsmk on weight change across levels of all of the other variables in the model. This constant effect assumption might be true, but if one wanted to account for potential interactions between the exposure and all of the confounders in the model, there is an easy way. We call this the "stratified modeling approach."

This stratified modeling approach avoids the exposure effect homogeneity assumption across levels of all the confounders. In effect, the approach fits a separate model for each exposure stratum. To obtain predictions under the “exposed” scenario, we use the model fit to the exposed individuals to generate predicted outcomes in the entire sample. To obtain predictions under the “unexposed” scenario, we repeat the same procedure, but with the model fit among the unexposed. One can then average the risks obtained under each exposure scenario, and take their difference and ratio to obtain the risk differences and ratios of interest.

```{r, warning = F, message = F}
#' Marginal Standardization
##' To avoid assuming no interaction between 
##' quitting smoking and any of the other variables
##' in the model, we subset modeling among 
##' exposed/unexposed. This code removes qsmk from the model,
##' which will allow us to regress the outcome 
##' against the confounders among the exposed and 
##' the unexposed separately. Doing so will allow us 
##' to account for any potential exposure-covariate interactions
##' that may be present. 
formulaVars <- "sex + age + income + sbp + dbp + price71 + tax71 + race"
modelForm <- as.formula(paste0("wt_delta ~", formulaVars))
modelForm

#' Regress the outcome against the confounders 
#' among the unexposed (model0) and then among the exposed (model1)
model0 <- glm(modelForm,data=subset(nhefs,qsmk==0),family=binomial("logit"))
model1 <- glm(modelForm,data=subset(nhefs,qsmk==1),family=binomial("logit"))
##' Generate predictions for everyone in the sample using the model fit to only the 
##' unexposed (mu0 predictions) and only the exposed (mu1 predictions).
mu1 <- predict(model1,newdata=nhefs,type="response")
mu0 <- predict(model0,newdata=nhefs,type="response")

#' Marginally adjusted odds ratio
marg_stand_OR <- (mean(mu1)/mean(1-mu1))/(mean(mu0)/mean(1-mu0))
#' Marginally adjusted risk ratio
marg_stand_RR <- mean(mu1)/mean(mu0)
#' Marginally adjusted risk difference
marg_stand_RD <- mean(mu1)-mean(mu0)

#' Using the bootstrap to obtain confidence intervals for the marginally adjusted 
#' risk ratio and risk difference.
bootfunc <- function(data,index){
  boot_dat <- data[index,]
  model0 <- glm(modelForm,data=subset(boot_dat,qsmk==0),family=binomial("logit"))
  model1 <- glm(modelForm,data=subset(boot_dat,qsmk==1),family=binomial("logit"))
  mu1 <- predict(model1,newdata=boot_dat,type="response")
  mu0 <- predict(model0,newdata=boot_dat,type="response")
  
  marg_stand_OR_ <- (mean(mu1)/mean(1-mu1))/(mean(mu0)/mean(1-mu0))
  marg_stand_RR_ <- mean(mu1)/mean(mu0)
  marg_stand_RD_ <- mean(mu1)-mean(mu0)
  res <- c(marg_stand_RD_,marg_stand_RR_,marg_stand_OR_)
  return(res)
}

#' Run the boot function. Set a seed to obtain reproducibility
set.seed(123)
boot_res <- boot(nhefs,bootfunc,R=2000)

boot_RD <- boot.ci(boot_res,index=1)
boot_RR <- boot.ci(boot_res,index=2)
boot_OR <- boot.ci(boot_res,index=3)

marg_stand_OR
marg_stand_RR
marg_stand_RD

boot_RD
boot_RR
boot_OR

```

When predicted risks are estimated using a logistic model, relying on marginal standardization will not result in probability estimates outside the bounds [0, 1]. And because the robust variance estimator is not required, model-based standardization will not be as affected by small sample sizes. However, the bootstrap is more computationally demanding than alternative variance estimators, which may pose problems in larger datasets.

\newpage

# References