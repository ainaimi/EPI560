---
title: "Estimating Regression Model Parameters: OLS and MLE"
author: "Ashley I Naimi"
date: "Spring 2022"
urlcolor: blue
bibliography: ref.bib
link-citations: yes
output: 
    bookdown::pdf_book:
      base_format: tint::tintPdf
      toc: true
      number_sections: true
      includes:
        in_header: "../misc/preamble.tex"
      latex_engine: xelatex
    html_document:
      theme: readable
      toc: true
      toc_float: true
      number_sections: true
      css: "../misc/style.css"
---

```{r setup, include=FALSE}
library(knitr)
opts_chunk$set(tidy.opts=list(width.cutoff=40),tidy=TRUE)

packages <- c( "data.table","tidyverse","ggplot2","ggExtra","formatR",
               "gridExtra","skimr","here","Hmisc","RColorBrewer")

for (package in packages) {
  if (!require(package, character.only=T, quietly=T)) {
    install.packages(package, repos='http://lib.stat.cmu.edu/R/CRAN',dependencies=T)
  }
}

for (package in packages) {
  library(package, character.only=T)
}

remotes::install_github("rstudio/fontawesome")

library(fontawesome)

thm <- theme_classic() +
  theme(
    legend.position = "top",
    legend.background = element_rect(fill = "transparent", colour = NA),
    legend.key = element_rect(fill = "transparent", colour = NA)
  )
theme_set(thm)
```

\newpage
\onehalfspacing

# Introduction

Regression is a cornerstone tool of any empirical analysis. It is arguably the most widely used tool in science. Regression models are often deployed for exploratory data analysis, to understand cause-effect relations between exposures and outcomes of interest, or to obtain predictions for an outcome of interest. 

# Least Squares Estimation

Consider a scenario in which we are interested in regressing an outcome $Y$ against a set of covariates $X$. \marginnote{Note that, in this formulation, the function of interest on the left hand side of the equation is the conditional mean function, $E(Y\mid X)$. However, there are other options, including hazards, failure times, distribution quantiles, and many more.} These covariates can be an exposure combined with a set of confounders needed for identification, or a set of predictors used to create a prediction algorithm via regression. In its most basic formulation, a regression model can be written as:

$$E(Y \mid X) = f(X)$$
In principle, this model is most flexible in that it states that the conditional mean of $Y$ is simply a *arbitrary function* of the covariates $X$. We are not stating (or assuming) precisely **how** the conditional mean is related to these covariates. Using this model, we might get predictions from to facilitate a decision making process, or obtain a contrast of expected means between two groups.

Because of the flexibility of this model, we may be interested in fitting it to a given dataset. But we can't. There is simply not enough information in this equation for us to quantify $f(X)$, even if we had all the data we could use. In addition to data, we need some "traction" or "leverage" to be able to quantify the function of interest.

```{r, warning = F, message = F, echo = F, include = F}

file_loc <- url("https://www.hsph.harvard.edu/miguel-hernan/wp-content/uploads/sites/1268/2019/03/nhefs.csv")

nhefs <- read_csv(file_loc) %>% 
  dplyr::select(qsmk,wt82_71,wt82, wt71, exercise,sex,age,
         race,income, marital,school,
         asthma,bronch, 
         starts_with("alcohol"),
         starts_with("price"),
         starts_with("tax"), 
         starts_with("smoke"),
         smkintensity82_71) %>% 
  mutate(income=as.numeric(income>15),
         marital=as.numeric(marital>2)) %>% 
  na.omit(.)

mod <- lm(wt82~wt71,data=nhefs)

plot1 <- nhefs %>% 
  sample_n(100) %>% 
  select(wt82, wt71) %>% 
  ggplot(.) +
  geom_abline(intercept = coef(mod)[1], slope = coef(mod)[2], col = "blue") +
  geom_segment(aes(y = coef(mod)[1] + coef(mod)[2] * wt71, 
                   yend = wt82, 
                   x = wt71, 
                   xend = wt71), col = "red") +
  geom_point(aes(x = wt71, y = wt82), size = 1, shape = 20) +
  scale_y_continuous(expand=c(0,0), limits=c(40,120)) +
  scale_x_continuous(expand=c(0,1), limits=c(40,120)) +
  xlab("x") + ylab("y")

ggsave(here("figures","2022_02_21-ssr_plot.pdf"), plot=plot1)

x <- seq(-3,3,.5)
plot2 <- ggplot() + 
  xlim(-3, 3) + 
  geom_function(fun = function(x) 7 + (0.57 - x)^2) +
  xlab("f(X)") + ylab("MSE[f(X)]")

ggsave(here("figures","2022_02_21-mse_optimization_plot.pdf"), plot=plot2)

plot3 <- gridExtra::grid.arrange(plot1,plot2,nrow=2)

ggsave(here("figures","2022_02_21-mse_ssr_plot.pdf"), 
       width = 7,
       height = 14,
       units = "cm",
       plot=plot3)

```

The earliest attempt to find some "traction" to quantify $f(X)$ was proposed in the 1800s [@Stigler1981, @Shalizi2019]. \marginnote{ {\em Mean squared error} can be be re-written as the sum of the squared bias and the variance: $$E[(Y - f(X))^2] = [E(Y) - f(X)]^2 + Var(Y),$$ which gives some insight as to what we are doing when we minimize mean squared error. In effect, we are finding the tradeoff between bias and variance for the model $f(X)$ given the data $X$.} The approach starts by accepting a few tenets. First, we want the difference between the observed $Y$ for any given individual and the fitted values $f(X)$ for that person to be "small." We also need a way to handle errors on both sides of the fitted values, so that two equal and opposite errors don't cancel out suggesting "zero" error. To address this, we can square the error and take it's average: $E[(Y - f(X))^2]$. Thus, we can define the "optimal" $f(X)$ as the function of $X$ that minimizes the mean squared error.

Recall from calculus that finding the $f(X)$ that minimizes mean squared error can be achieved by taking the derivative of the mean squared error with respect to $f(X)$, setting it to zero, then solving for $f(X)$.

```{r ssrmseplot, out.width="5cm", fig.align='center', fig.margin=F, echo=F, fig.cap="Line of 'best fit' (blue line) defined on the basis of minimizing the sum of squared residuals (red lines) displayed in the top panel; Partial representation of the mean squared error as a function of f(X) in the bottom panel. The lowest mean squared error value corresponds to the function f(X) represented by the blue line of 'best fit' in the top panel."}
knitr::include_graphics(here("figures","2022_02_21-mse_ssr_plot.pdf"))
```

We've made some progress, but without a better sense of what $f(X)$ looks like, we still can't move forward. For example, there are several functions where either the derivative simply does not exist (e.g., if $f(X)$ is discontinuous), or where the derivative is still complex enough that we can't make progress with finding a unique solution for $f(X)$ that minimizes mean squared error (see technical note on nonlinear models).

Early on, it was recognized that if we select $f(X)$ to be *linear* (more technically, *affine*) the problem of finding the optimal $f(X)$ becomes much easier. That is, if we can simplify $f(X) = b_0 + b_1 X$, then we can use calculus and simple algebra to find an optimal *linear* solution set $b_0 = \beta_0, b_1 = \beta_1$ that minimizes MSE. 

Specifically, we can re-write the mean squared error as a function of $b_0 + b_1 X$ to be $MSE(b_0,b_1) = E[(Y - (b_0 + b_1 X))^2]$. Taking the partial derivatives of $MSE(b_0, b_1)$ with respect to $b_0$ and $b_1$ gives us the ordinary least squares estimator for the coefficients in the model [@Rencher2000, @Shalizi2019]:

\begin{align*}
\hat{b}_1 & = \frac{\sum_i (x_i - \bar{x})(y_i - \bar{x})}{\sum_i (x_i - \bar{x})^2} \\
\hat{b}_0 & = \bar{y} - \hat{b}_1\bar{x}
\end{align*}

We can see these equations in action in an actual dataset. Let's conduct a simple exploratory analysis of the relation between weight (in kg) in 1982 and in 1971 among observations in the NHEFS data:

```{r tidy = F, warning = F, message = F}

# locate the data on the website
file_loc <- url("https://bit.ly/47ECRcs")

# load the data
nhefs <- read_csv(file_loc) %>% 
  dplyr::select(wt82, wt71) %>% 
  na.omit(.)

# construct the numerator and denomiantor of the OLS estimator for b1
num <- sum((nhefs$wt71 - mean(nhefs$wt71))*(nhefs$wt82 - mean(nhefs$wt71)))
den <- sum((nhefs$wt71 - mean(nhefs$wt71))^2)

b1 <- num/den

# use the estimate of b1 to compute b0
b0 <- mean(nhefs$wt82) - b1*mean(nhefs$wt71)

# compare the b1 and b0 estimates above to what we get using OLS via 
# the lm function in R
mod <- lm(wt82 ~ wt71, data=nhefs)

b0
b1

summary(mod)$coefficients

```

<!-- In the way we formulated the model above, the set of linear models we can use to find a solution $f(X)$ that minimizes $MSE(f(X))$ is limited. For instance, in the case where $Y$ is binary, and $E(Y) = P(Y=1)$, using a linear model such as $b_0 + b_1 X$ can easily lead to problems, most notably that predicted probabilities lie outside of the bounds $[0, 1]$.  -->

:::{.rmdnote data-latex="{note}"}

__Technical Note__:

|               Technically (almost to the point of pedantry), a nonlinear model is a model where the first derivative of the expectation taken with respect to the parameters is itself a function of other parameters [@Seber1989]. For example,

$$ E(Y \mid X) = \beta_0 + \frac{X}{\beta_1}   $$
is a nonlinear model, because it's first derivative taken with respect to $\beta_1$ is still a function of $\beta_1$. Recalling that the derivative of $\tfrac{1}{X} = \tfrac{1}{X^2}$:

$$\frac{d E(Y \mid X)}{d \beta_1} = \frac{d \left (\beta_0 + \frac{X}{\beta_1} \right ) }{d \beta_1} =  - \frac{X}{\beta_1^2}$$

The resolution to this specific example of nonlinearity is simple. We can define $\alpha = \tfrac{1}{\beta_1}$, and fit a linear model:

$$ E(Y \mid X) = \beta_0 + \alpha X $$
and then obtain an estimate of $\beta_1$ as $\tfrac{1}{\hat{\alpha}}$. 

However, this simple nonlinear model illustrates an important technical distinction between linear and nonlinear models. Why is this important? Solutions to these regression equations (which serve as our estimates), are obtained by finding where the slope of the tangent line of the parameters is zero. To do this, we need to set the first derivative of these regression equations to zero. But if there are still parameters in these first derivative equations, then there will not be a unique solution to the equation, and finding an estimate will require more complex approaches. This is the complication introduced by nonlinear models. 

On the other hand, curvilinear models are linear models whose relationships can't be adequately captured by straight lines. These are easy to find solutions for, since their first derivatives are not functions of parameters. For instance, for a quadratic model such as:
$$E(Y \mid X) = \beta_0 + \beta_1 X + \beta_2 X^2$$
The first derivatives taken with respect to each parameter turn out to be:
$$ \frac{d E(Y \mid X, C)}{d \beta_0} = 1$$
$$ \frac{d E(Y \mid X, C)}{d \beta_1} = X$$
$$ \frac{d E(Y \mid X, C)}{d \beta_2} = X^2$$

Thus, even though the regression function will not be a "straight line" on a plot, this model is still linear.

:::

<!-- There are some important points to note in how we formulated the problem of estimating $E(Y \mid X) = f(X)$ with a linear model: -->

<!-- - What we needed to invoke to make this work is that a linear approximation to $f(X)$ is "good enough" for our interest. We need the linear approximation so we can take derivatives of the MSE function without running into problems. These assumptions are usually referred to as "regularity" conditions [@Longford2008]. -->

<!-- - We didn't explicitly state it, but on the basis of Figure 1, this approach makes most sense if $Y$ is continuous. If $Y$ is binary, categorical, or time-to-event, the rationale motivating this approach starts to break down to a degree. That is, while it is possible to fit an OLS estimator to a binary outcome data, not everyone agrees that this is a good idea, and caution is warranted when doing so. -->

<!-- - We *did not* need to invoke any any assumptions about homoscedasticity, or independent and identically distributed (iid) observations. If we were able to make these assumptions, then we obtain an estimator that is the "best linear unbiased estimator" [@Rencher2000].  -->

<!-- - We *did not* need to invoke any distributional assumptions about $Y$ (or, more specifically, the conditional mean of $Y$). If we can assume Gaussian with constant variance, then we can equate the OLS estimator with the maximum likelihood estimator with a Gaussian distribution and identify link function. -->



# Maximum Likelihood Estimation

## Basic Illustration

Maximum likelihood estimation is another more general technique that we can use to fit a model $f(X)$ to data. To illustrate, let's start with a simpler example where we want to estimate the 10 day risk of diarrhea ($Y \in [0,1]$) among 30 infants infected with *Vibrio cholerae* who are also being breastfed. The scientific question of interest is the role that antibiotic concentrations in breastmilk ($X \in [0 = \text{low}, 1 = \text{high}]$) can play in reducing the incidence of diarrheal disease. The data are in Table 1 [taken from @Cole2013]:

| Antibiotic Level | Cases ($Y = 1$) | non-Cases ($Y = 0$) | Total|
|:-----------------|----:|---:|----:|
| Low ($X = 0$)    | 12  |   2|  14|
| High ($X = 1$)   | 7   |   9|  16|
| Total            | 19  |  11|  30|

<!-- Because diarrhea occurrence is a binary event, we can define each individual's probability of experiencing a case of diarrhea $p$ as a Bernoulli random variable: -->

<!-- $$P(Y_i = 1) = p^y(1 - p)^{1 - y}$$ -->
<!-- For two infants in the sample, if their probability of experiencing diarrhea is not correlated, then we can write the probability that they both experience diarrhea as: -->

<!-- $$P(Y_i = 1, Y_j = 1) = [p^{y_i}(1 - p)^{1 - y_i}] \times [p^{y_j}(1 - p)^{1 - y_j}] $$ -->

If we assume that diarrhea was an independent occurrence across the 30 infants, we can define its probability mass function as:

\begin{equation}
P(Y = y) = {n \choose y}p^{y}(1 - p)^{n - y} \label{probeq}
\end{equation}

\marginnote{In a direct problem, the known portion $p$ is held fixed, and when we change the data inputs we obtain different probabilities of seeing $y$ cases of diarrhea out of $n$ observations.} \noindent In this case, $p$ is the probability of observing a single diarrheal event. For the moment, let's ignore the fact that we have exposed and unexposed infants in the cohort that this equation is meant to represent. We are working in an indirect problem setting (we have the data, we don't know the underlying probability $p$). This will enable us to re-write equation \ref{probeq} as:

$$P(Y = y) = {30 \choose 19}p^{19}(1 - p)^{30 - 19}$$

\noindent which still doesn't help us, because all we have is data. However, if we guessed that $p = 0.6$, we could then get a predicted outcome:

$$P(Y = y {\color{red}; p = 0.6}) = 0.14 = {30 \choose 19}0.6^{19}(1 - 0.6)^{30 - 19}$$

This predicted value of 0.14 is no longer a probability, because the variable in the original equation is $p$, and not (as would usually be the case) the data (i.e., the *variables*). Instead, we call this the **likelihood of the parameter** $p$. We can try another guess at the parameter, say $p = 0.4$:

$$P(Y = y {\color{red}; p = 0.4}) = 0.005 = {30 \choose 19}0.4^{19}(1 - 0.4)^{30 - 19}$$

Here, the likelihood of this parameter is much lower than the first. What does this suggest?: if the equation governing the data generating mechanism is correct, then the data are **more compatible** with $p = 0.6$ than they are with $p = 0.4$. Thus, it is more likely that the true $p$ is closer to 0.6 than it is to 0.6, given the data we have.

Let's look at this likelihood function a bit more systematically:

```{r tidy = F, warning = F, message = F}

likelihood_res <- NULL
for (i in seq(0,1,.01)){
  likelihood_res <- rbind(
    likelihood_res,
    cbind(choose(30,19)*(i^19)*((1 - i)^(30 - 19)), i)
  )
}

likelihood_res <- data.frame(likelihood_res)

names(likelihood_res) <- c("likelihood", "parameter")

ggplot(likelihood_res) +
  geom_point(aes(x = parameter, 
                 y = likelihood)) +
  geom_vline(xintercept = 19/30, color = "red")

```

This likelihood function tells us that the most likely parameter value is `r round(19/30, 3)`.

The key takeaway is that maximum likelihood estimation relies on a distribution function (probability mass function or probability density function) for the data generating mechanism. This distribution function is usually used in the context of direct problems to compute the probability of an event or set of events under a known set of parameters defining the distribution:

$$f(y; \theta)$$
However, in maximum likelihood estimation, this relationship is flipped. The likelihood function usually looks exactly the same as the distribution function, but it's used differently. Here, we treat the data as fixed, and try to find the parameters with the highest likelihood given the data:

$$L(\theta; y)$$
## MLE for Regression

How might this work if we are interested in understanding the role that antibiotic concentrations in breastmilk play in the incidence of diarrhea? 

Like in our OLS example, if we assume independence across observations, we can define the probability mass function for observing $y$ cases of diarrhea among those with $X = x$ as:

$$P(Y = y \mid X = x) = {n_x \choose y_x}p_x^{y_x}(1 - p_x)^{n - y_x}$$
\noindent where (again) $X \in [0, 1]$. However, this time we can define $p_x = \expit [\beta_0 + beta_1 x]$ where $\expit(a) = \tfrac{1}{[1 + \exp(-a)]}$. Now that we've made $p_x$ a function of $\beta_0$ and $beta_1$, we can define the likelihood function here as:

$$L(\beta_0, \beta_1;y_x) = \prod_{x = 0,1}{n_x \choose y_x}p_x^{y_x}(1 - p_x)^{n_x - y_x}$$

Once again, the task is to find values of $\beta_0$ and $beta_1$ that maximize this likelihood function, and that are thus most compatible with the data. We could plug different values in and evaluate the likelihood as we did above. However, in practice, we would once again use calculus to find where the slope of the likelihood function is equaled to zero. 

To make the math easier, we often simplify the likelihood function by ignoring factors like $n \choose y$, since the derivative of the likelihood function will not depend on this scaling factor. Furthermore, if we take the log of the likelihood function, computing derivatives is much simpler:

$$\mathcal{L}(\beta_0, \beta_1;y_x) = \ln[L(\beta_0, \beta_1;y_x)] = \prod_{x = 0,1} y_x \ln p_x + (n_x - y_x) \ln (1 - p_x)$$
The (partial) derivatives of this log-likelihood function with respect to the parameters is often referred to as the score function, the gradient, or (less commonly) the informant. If we set the score function for each parameter to zero and solve for the parameter values, we get the score equations for $\beta_0$ and $\beta_1$, which are our maximum likelihood estimators. In this simple example, solutions for the score equations are easy to compute, and become [@Cole2013]:

$$\hat{\beta}_0 =  \ln \left ( \frac{y_0}{n_0 - y_0} \right ) = \ln(7/9) = -0.25$$
$$\hat{\beta}_1 =  \ln \left [ \frac{y_1(n_0 - y_0)}{y_0(n_1 - y_1)} \right ] = \ln \left [\frac{12 \times (16 - 7)}{7\times (14 - 12)}  \right ] = 2.04$$

We can again compare these to what we would get from an actual regression analysis of these data:

```{r tidy = F, warning = F, message = F}

d <- data.frame(
  y = c(1, 1, 0, 0),
  x = c(1, 0, 1, 0), 
  freq = c(12, 7, 2, 9)
)

mod_glm <- glm(y ~ x, data = d, weights = freq, family = binomial("logit"))

summary(mod_glm)$coefficients

```


<!-- We elect to constrain $p = f(X)$ so that the predictions are forced to lie within $[0, 1]$. For instance, we could use the inverse of the logistic function and define $f(X) = \frac{1}{1 + \exp(-b_0 - b_1 X)}$. However, in this case the derivatives would no longer work out as simply as we'd need them too (see Technical Note). In effect, we would now have to deal with the fact that the model is nonlinear (i.e., the derivatives of the model cannot simply be set to zero). -->

<!-- As it turns out, in the early 1970's, Nelder and Wedderburn [@Nelder1972] made a seminal contribution that enabled fitting nonlinear models when the outcome belongs to the exponential family of distributions,^[The exponential family of distributions is not to be confused with the exponential distribution. It refers to a family of distributions that can be re-written such that they can be represented in a common form. These distributions include (but are not limited to) the Gaussian, exponential, bernoulli (binomial), Poisson, and negative binomial.] and the conditional mean of the outcome can be linked to the covariates through some smooth and invertible linearizing function (which includes the $\log$ and $\logit$ functions). -->


\newpage

# References