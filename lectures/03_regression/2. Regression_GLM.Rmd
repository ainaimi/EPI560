---
title: "Generalized Linear Models: Distributions and Link Functions"
author: "Ashley I Naimi"
date: "`r paste0('Spring ', format(Sys.Date(), '%Y'))`"
urlcolor: blue
bibliography: ref.bib
link-citations: yes
output: 
    bookdown::pdf_book:
      base_format: tint::tintPdf
      toc: true
      number_sections: true
      includes:
        in_header: "../misc/preamble.tex"
      latex_engine: xelatex
    html_document:
      theme: readable
      toc: true
      toc_float: true
      number_sections: true
      css: "../misc/style.css"
---

```{r setup, include=FALSE}
library(knitr)
opts_chunk$set(tidy.opts=list(width.cutoff=40),tidy=TRUE)

packages <- c( "data.table","tidyverse","ggplot2","ggExtra","formatR", "kableExtra",
               "gridExtra","skimr","here","Hmisc","RColorBrewer")

for (package in packages) {
  if (!require(package, character.only=T, quietly=T)) {
    install.packages(package, repos='http://lib.stat.cmu.edu/R/CRAN',dependencies=T)
  }
}

for (package in packages) {
  library(package, character.only=T)
}

remotes::install_github("rstudio/fontawesome")

library(fontawesome)

thm <- theme_classic() +
  theme(
    legend.position = "top",
    legend.background = element_rect(fill = "transparent", colour = NA),
    legend.key = element_rect(fill = "transparent", colour = NA)
  )
theme_set(thm)
```

\newpage
\onehalfspacing

# Generalized Linear Models

Generalized linear models consist of a family of regression models that are fully characterized by a selected distribution and a link function. That is, to fully specify a GLM, one must select a distribution (which determines the form of the conditional mean and variance of the outcome) and a link function (which determines how the conditional mean of the outcome relates to the covariates).

There are a wide variety of distributions and link functions available in standard statistical software programs that fit GLMs. Here, we'll consider a binary outcome $Y$ with probability $P(Y=1)$, and focus attention on three link functions: 

1. Logit, or the log-odds: $\log{P(Y=1)/[1-P(Y=1)]}$
2. Log: $\log[P(Y=1)]$
3. Identity: $P(Y=1)$.

A common misconception is that to use GLMs correctly, one must choose the distribution that best characterizes the data (the "correct" distribution), as well as the canonical link function corresponding to this distribution. For example, if the outcome is binary, one "must" choose the binomial distribution with the logit link. While the binomial distribution and logit link work well together for binary outcomes, they do not easily provide contrasts like the risk difference or risk ratio, because of the selected link function. Alternative specification of the distribution and link function for GLMs can address this limitation.

# Link Functions and Effect Measures

There is an important relation between the chosen link function, and the interpretation of the coefficients from a GLM. For models of a binary outcome and the logit or log link, this relation stems from the properties and rules governing the natural logarithm. The quotient rule states that $\log(X/Y) = \log(X) âˆ’ \log(Y)$.

Because of this relation, the natural exponent of the coefficient in a logistic regression model yields an estimate of the odds ratio. Too see why, we can evaluate the logit link function in a regression model:

\begin{align*}
& \log \left [ \frac{P( Y = 1 \mid X)}{P( Y = 0 \mid X)} \right ] = \beta_0 + \beta_1 X \\
\implies &  \beta_1 = \log \left [ \frac{P( Y = 1 \mid X = 1)}{P( Y = 0 \mid X = 1)} \right ] - \log \left [ \frac{P( Y = 1 \mid X = 0)}{P( Y = 0 \mid X = 0)} \right ] \\
\implies &  \beta_1 = \log \left [ \tfrac{P( Y = 1 \mid X = 1)}{P( Y = 0 \mid X = 1)} \bigg / \tfrac{P( Y = 1 \mid X = 0)}{P( Y = 0 \mid X = 0)} \right ] \\
\implies &  \exp(\beta_1) = \tfrac{P( Y = 1 \mid X = 1)}{P( Y = 0 \mid X = 1)} \bigg / \tfrac{P( Y = 1 \mid X = 0)}{P( Y = 0 \mid X = 0)} 
\end{align*}


However, by the same reasoning, exponentiating the coefficient from a GLM with a log link function and a binomial distribution (i.e., log-binomial regression) yields an estimate of the risk ratio:

\begin{align*}
& \log \left [ P( Y = 1 \mid X) \right ] = \beta_0 + \beta_1 X \\
\implies &  \beta_1 = \log \left [ P( Y = 1 \mid X = 1) \right ] - \log \left [ P( Y = 1 \mid X = 0) \right ] \\
\implies &  \beta_1 = \log \left [ \frac{P( Y = 1 \mid X = 1)}{P( Y = 1 \mid X = 0)} \right ] \\
\implies &  \exp(\beta_1) = \frac{P( Y = 1 \mid X = 1)}{P( Y = 1 \mid X = 0)}
\end{align*}

Alternately, for GLM models with a binomial distribution and identity link function, because logarithms are not used, the unexponentiated coefficient yields an estimate of the risk difference:

\begin{align*}
& \left [ P( Y = 1 \mid X) \right ] = \beta_0 + \beta_1 X \\
\implies &  \beta_1 = \left [ P( Y = 1 \mid X = 1) \right ] - \left [ P( Y = 1 \mid X = 0) \right ]
\end{align*}

Unfortunately, using a binomial distribution can lead to convergence problems with the $\log()$ or identity link functions [@Zou2004]. This will occur when, for example, the combined numerical value of all the independent variables in the model is large enough to cause the estimated probabilities to exceed 1, which violates the very definition of a probability (binomial) model (probabilities can only lie between zero and one) and hence, convergence problems. Let's see how these problems can be overcome.

# A Data Example

We use data from the National Health and Nutrition Examination Survey (NHEFS). We are interested primarily in the covariate adjusted association (on the risk difference and risk ratio scales) between quitting smoking and a greater than median weight change between 1971 and 1982.

In our analyses, we regress an indicator of greater than median weight change against an indicator of whether the person quit smoking. We adjust for exercise status, sex, age, race, income, marital status, education, and indicators of whether the person was asthmatic or had bronchitis. We start by loading the data:

```{r, warning = F, message = F}
#' Load relevant packages
packages <- c("broom","here","tidyverse","skimr","rlang","sandwich","boot", "kableExtra")

for (package in packages) {
  if (!require(package, character.only=T, quietly=T)) {
    install.packages(package, repos='http://lib.stat.cmu.edu/R/CRAN')
  }
}

for (package in packages) {
  library(package, character.only=T)
}

#' Define where the data are
file_loc <- url("https://bit.ly/47ECRcs")

#' This begins the process of cleaning and formatting the data
nhefs <- read_csv(file_loc) %>% 
  select(qsmk,wt82_71,wt82, wt71, exercise,sex,age,
         race,income, marital,school,
         asthma,bronch, 
         starts_with("alcohol"),-alcoholpy,
         starts_with("price"),
         starts_with("tax"), 
         starts_with("smoke"),
         smkintensity82_71) %>% 
  mutate(income=as.numeric(income>15),
         marital=as.numeric(marital>2),
         alcoholfreq=as.numeric(alcoholfreq>1)) %>% 
  na.omit(.)

factor_names <- c("exercise","income","marital",
                  "sex","race","asthma","bronch")
nhefs[,factor_names] <- lapply(nhefs[,factor_names] , factor)

#' Define outcome
nhefs <- nhefs %>% mutate(id = row_number(), 
                          wt_delta = as.numeric(wt82_71>median(wt82_71)),
                          .before = qsmk)

#' Quick summary of data
nhefs %>% print(n=5)
```


# GLMs for Risk Differences and Ratios

For our analyses of the data described above using GLM with a binomial distributed outcome with a log link function to estimate the risk ratio and identity link function to estimate risk difference, an error is returned:

```{r tidy = F, error = T}
#' Here, we start fitting relevant regression models to the data.
#' modelForm is a regression argument that one can use to regress the 
#' outcome (wt_delta) against the exposure (qsmk) and selected confounders.

formulaVars <- paste(names(nhefs)[c(3,7:16)],collapse = "+")
modelForm <- as.formula(paste0("wt_delta ~", formulaVars))
modelForm

#' This model can be used to quantify a conditionally adjusted 
#' odds ratio with correct standard error
modelOR <- glm(modelForm,data=nhefs,family = binomial("logit"))
tidy(modelOR)[2,]

#' This model can be used to quantify a conditionally adjusted risk 
#' ratio with with correct standard error
#' However, error it returns an error and thus does not provide any results.
modelRR_binom <- glm(modelForm,data=nhefs,family = binomial("log"))
```

Why is this error returned? We are modeling $P(Y = 1 \mid X) = \exp\{\beta_0 + \beta_1 X_1 + \ldots + \beta_p X_p\}$. In this context, there may be *no set of values* for the parameters in the model that yield $P(Y = 1 \mid X) < 1$ for every observation in the sample. Because R's glm function (under a binomial distribution) correctly recognizes this as a problem, it returns an error.

The most commonly proposed solution is to follow the error's advice and provide starting values to initiate the algorithm, but there are two problems with this. The first is that it often doesn't work:

```{r tidy = F, error = T}

#' This model can be used to quantify a conditionally adjusted risk 
#' ratio with with correct standard error
#' It adds starting values for the parameters in an attempt to 
#' get the model to converge.
#' However, it still returns an error.
modelRR_binom <- glm(modelForm,
                     data=nhefs,
                     family = binomial("log"),
                     start = rep(0, 13))
```

The second problem is that, even if it does work, the results may be strongly dependent on the starting values of the algorithm. While this is not a fundamental problem, it can create some challenges in interpreting the results we obtain. 

Instead, one may resort to using different distributions that are more compatible with the link functions that return the association measures of interest. For the risk ratio, one may use a GLM with a Poisson distribution and log link function. Doing so will return an exposure coefficient whose natural exponent can be interpreted as a risk ratio. 

```{r}
#' This model can be used to quantify a conditionally risk ratio 
#' using the Poisson distribuiton and log link function. 
#' However, because the Poisson distribution is used, the model 
#' provides incorrect standard error estimates.
modelRR <- glm(modelForm,data=nhefs,family = poisson("log"))
tidy(modelRR)[2,]
```

It's important to recognize what we're doing here. We are using this model as a tool to quantify the log mean ratio contrasting $P(Y = 1 \mid X_{qsmk} = 1)$ to $P(Y = 1 \mid X_{qsmk} = 0)$ (all other things being equal). However, we should not generally assume that ever aspect of this model is correct. In particular, note that the max predicted probability from this model is `r round(max(modelRR$fitted.values), 3)`:

```{r, message=F, warning=F}
summary(modelRR$fitted.values)
```

We can use the `augment` function in the `broom` package to evaluate the distribution of these probabilities (among other things):
```{r, message=F, warning=F}
fitted_dat <- augment(modelRR, type.predict="response")

fitted_dat

plot_hist <- ggplot(fitted_dat) +
  geom_histogram(aes(.fitted)) +
  scale_y_continuous(expand=c(0,0)) +
  scale_x_continuous(expand=c(0,0))

ggsave(here("figures","2022_02_21-rr_hist_plot.pdf"), plot=plot_hist)
```

This distribution is shown in margin Figure \ref{fig:fittedhist}. We can also see that there are only two observations in the sample with predicted risks greater than 1.

```{r fittedhist, out.width="5cm", fig.align='center', fig.margin=TRUE, echo=F, fig.cap="Distribution of fitted values from the Poisson GLM with log link function to obtain an estimate of the adjusted risk ratio for the association between quitting smoking and greater than median weight gain in the NHEFS."}
knitr::include_graphics(here("figures","2022_02_21-rr_hist_plot.pdf"))
```

```{r}
fitted_dat %>% 
  filter(.fitted >= 1) %>% 
  select(wt_delta, qsmk, age, .fitted)
```

For these reasons, we are not particularly concerned about the fact that the model predicts risks that are slightly large than 1. However, the model-based standard errors (i.e., the SEs that one typically obtains directly from the GLM output) are no longer valid. Instead, one should use the robust (or sandwich) variance estimator to obtain valid SEs (the bootstrap can also be used) [@Zou2004]. The easiest way to do this in R is to use the `lmtest` and the `sandwich` packages:

```{r tidy = F}
library(lmtest)
library(sandwich)
#' To obtain the correct variance, we use the "sandwich" 
#' function to obtain correct sandwich (robust) standard 
#' error estimates.
coeftest(modelRR, vcov = sandwich(modelRR))

```

For the risk difference, one may use a GLM with a Gaussian (i.e., normal) distribution and identity link function, or, equivalently, an ordinary least squares estimator. Doing so will return an exposure coefficient that can be interpreted as a risk difference. However, once again the robust variance estimator (or bootstrap) should be used to obtain valid SEs.

```{r}
#' This model can be used to obtain a risk difference 
#' with the gaussian distribiton or using ordinary least 
#' squares (OLS, via the lm function). Again, the model 
#' based standard error estimates are incorrect. 
modelRD <- glm(modelForm,data=nhefs,family = gaussian("identity"))
modelRD <- lm(modelForm,data=nhefs)
tidy(modelRD)[2,]
#' To obtain the correct variance, we use the "sandwich" function
#' to obtain correct sandwich (robust) standard error estimates.
coeftest(modelRD, vcov = sandwich(modelRD))
```

The risk ratio and difference, as well as the 95% sandwich variance confidence intervals, obtained for the relation between quitting smoking and greater than median weight change are provided Table 1.

```{r, echo = F}
table1_data <- tibble(Method = c("GLM","Marginal Standardization"),
                      `Risk Difference` = c("0.14	(0.09, 0.20)", "0.14	(0.09, 0.21)"), 
                      `Risk Ratio` = c("1.32	(1.19, 1.46)", "1.31	(1.18, 1.46)"))
```

```{r}
knitr::kable(table1_data)
```

\

Results in this table obtained using a conditionally adjusted regression model without interactions. Gaussian distribution and identity link was used to obtain the risk difference. A Poisson distribution and log link was used to obtain the risk ratio. 95% CIs obtained via the sandwich variance estimator. 95% CIs obtained using the bias-corrected and accelerated bootstrap CI estimator.

Unfortunately, use of a Poisson or Gaussian distribution for GLMs for a binomial outcome can introduce different problems. For one, while not entirely worrysome in our setting, a model that predicts probabilities greater than one should not instill confidence in the user. Second, performance of the robust variance estimator is notoriously poor with small sample sizes. Finally, the interpretation of the risk differences and ratios becomes more complex when the exposure interacts with other variables in the model. 

\newpage

```{r, echo = F}
table2_data <- tibble(`Odds Ratio` = c("GLM Family = Binomial", "GLM Link = Logistic", "Standard Errors = Model Based", " ", " ", " ", " ", " "), 
                      `Risk Ratio` = c("GLM Family = Binomial", "GLM Link = Log", "Standard Errors = Model Based", "GLM Family = Poisson", "GLM Link = Log", "Standard Errors = Sandwich", " ", " "),
                      `Risk Difference` = c("GLM Family = Binomial", "GLM Link = Identity", "Standard Errors = Model Based", "GLM Family = Gaussian", "GLM Link = Identity", "Standard Errors = Sandwich", "Least Squares Regression", "Standard Errors = Sandwich"))
```

```{r, echo=F}
knitr::kable(table2_data, "simple", caption = "Methods to use for quantifying conditionally adjusted odds ratios, risk ratios, and risk differences.")
```

For these reasons, marginal standardization should be generally considered as a first line estimator of contrasts of interest. When predicted risks are estimated using a logistic model, relying on marginal standardization will not result in probability estimates outside the bounds [0, 1]. And because the robust variance estimator is not required, model-based standardization will not be as affected by small sample sizes. However, the bootstrap is more computationally demanding than alternative variance estimators, which may pose problems in larger datasets.

\newpage

# References