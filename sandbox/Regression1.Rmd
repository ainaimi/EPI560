---
title: "Regression in Time-Fixed Settings: Part 1"
author: "Ashley I Naimi"
date: "Spring 2022"
urlcolor: blue
bibliography: ref.bib
link-citations: yes
output: 
    bookdown::pdf_book:
      base_format: tint::tintPdf
      toc: true
      number_sections: true
      includes:
        in_header: "../misc/preamble.tex"
      latex_engine: xelatex
    html_document:
      theme: readable
      toc: true
      toc_float: true
      number_sections: true
      css: "../misc/style.css"
---

```{r setup, include=FALSE}
library(knitr)
opts_chunk$set(tidy.opts=list(width.cutoff=40),tidy=TRUE)

packages <- c( "data.table","tidyverse","ggplot2","ggExtra","formatR",
               "gridExtra","skimr","here","Hmisc","RColorBrewer")

for (package in packages) {
  if (!require(package, character.only=T, quietly=T)) {
    install.packages(package, repos='http://lib.stat.cmu.edu/R/CRAN',dependencies=T)
  }
}

for (package in packages) {
  library(package, character.only=T)
}

remotes::install_github("rstudio/fontawesome")

library(fontawesome)

thm <- theme_classic() +
  theme(
    legend.position = "top",
    legend.background = element_rect(fill = "transparent", colour = NA),
    legend.key = element_rect(fill = "transparent", colour = NA)
  )
theme_set(thm)
```

\newpage
\onehalfspacing

\newpage
\onehalfspacing

# Introduction to Regression

Regression is a cornerstone tool of any empirical analysis. It is arguably the most widely used tool in science. Regression models are often deployed in an attempt to understand cause-effect relations between exposures and outcomes of interest, or to obtain predictions for an outcome of interest. Consider a scenario in which we are interested in regressing an outcome $Y$ against a set of covariates $X$. These covariates can be an exposure combined with a set of confounders needed for identification, or a set of predictors used to create a prediction algorithm via regression. In its most basic (?) formulation, a regression model can be written as^[Note that, in this formulation, the function of interest on the left hand side of the equation is the conditional mean function, $E(Y\mid X)$. However, there are other options, including hazards, failure times, distribution quantiles, and many more.]:

$$E(Y \mid X) = f(X)$$
In principle, this model is most flexible in that it states that the conditional mean of $Y$ is simply a *arbitrary function* of the covariates $X$. We are not stating (or assuming) precisely **how** the conditional mean is related to these covariates. Using this model, we might get predictions from to facilitate a decision making process, or obtain a contrast of expected means between two groups.

Because of the flexibility of this model, we may be interested in fitting it to a given dataset. But we can't. There is simply not enough information in this equation for us to quantify $f(X)$, even if we had all the data we could use. In addition to data, we need some "traction" or "leverage" to be able to quantify the function of interest.

```{r, warning = F, message = F, echo = F, include = F}
file_loc <- url("https://cdn1.sph.harvard.edu/wp-content/uploads/sites/1268/1268/20/nhefs.csv")

nhefs <- read_csv(file_loc) %>% 
  select(qsmk,wt82_71,wt82, wt71, exercise,sex,age,
         race,income, marital,school,
         asthma,bronch, 
         starts_with("alcohol"),
         starts_with("price"),
         starts_with("tax"), 
         starts_with("smoke"),
         smkintensity82_71) %>% 
  mutate(income=as.numeric(income>15),
         marital=as.numeric(marital>2)) %>% 
  na.omit(.)

mod <- lm(wt82~wt71,data=nhefs)

plot1 <- nhefs %>% 
  sample_n(100) %>% 
  select(wt82, wt71) %>% 
  ggplot(.) +
  geom_abline(intercept = coef(mod)[1], slope = coef(mod)[2], col = "blue") +
  geom_segment(aes(y = coef(mod)[1] + coef(mod)[2] * wt71, 
                   yend = wt82, 
                   x = wt71, 
                   xend = wt71), col = "red") +
  geom_point(aes(x = wt71, y = wt82), size = 1, shape = 20) +
  scale_y_continuous(expand=c(0,0), limits=c(40,120)) +
  scale_x_continuous(expand=c(0,1), limits=c(40,120)) +
  xlab("x") + ylab("y")

ggsave(here("figures","2022_02_21-ssr_plot.pdf"), plot=plot1)

x <- seq(-3,3,.5)
plot2 <- ggplot() + 
  xlim(-3, 3) + 
  geom_function(fun = function(x) 7 + (0.57 - x)^2) +
  xlab("f(X)") + ylab("MSE[f(X)]")

ggsave(here("figures","2022_02_21-mse_optimization_plot.pdf"), plot=plot2)

plot3 <- gridExtra::grid.arrange(plot1,plot2,nrow=2)

ggsave(here("figures","2022_02_21-mse_ssr_plot.pdf"), 
       width = 7,
       height = 14,
       units = "cm",
       plot=plot3)

```

The earliest attempt to find some "traction" to quantify $f(X)$ was proposed in the 1800s. The approach starts by accepting a few tenets^[Much of this section is based on the (excellent) forthcoming (?) book by Cosma @Shalizi2019.]. First, we want the difference between the observed $Y$ for any given individual and the fitted values $f(X)$ for that person to be "small." If we didn't care about the direction of these errors (we usually don't), we can square the error and take it's average: $E[(Y - f(X))^2]$. Thus, we can define the "optimal" $f(X)$ as the function of $X$ that minimizes the mean squared error.

*Mean squared error* can be be re-written as the sum of the squared bias and the variance: 

$$E[(Y - f(X))^2] = [E(Y) - f(X)]^2 + Var(Y)$$
And you may recall that finding the $f(X)$ that minimizes mean squared error can be achieved by taking the derivative of the mean squared error with respect to $f(X)$, setting it to zero, then solving for $f(X)$.

```{r ssrmseplot, out.width="5cm", fig.align='center', fig.margin=TRUE, echo=F, fig.cap="Line of 'best fit' (blue line) defined on the basis of minimizing the sum of squared residuals (red lines) displayed in the top panel; Partial representation of the mean squared error as a function of f(X) in the bottom panel."}
knitr::include_graphics(here("figures","2022_02_21-mse_ssr_plot.pdf"))
```

We've made some progress, but without a better sense of what $f(X)$ looks like, we still can't move forward. For example, there are several functions where either the derivative simply does not exist (e.g., if $f(X)$ is discontinuous), or where the derivative is still complex enough that we can't make progress with finding a unique solution for $f(X)$ that minimizes mean squared error (see technical note on nonlinear models).

Early on, it was recognized that if we select $f(X)$ to be *linear* (more technically, *affine*) the problem of finding the optimal $f(X)$ becomes much easier. That is, if we can simplify $f(X) = b_0 + b_1 X$, then we can use calculus and simple algebra to find an optimal *linear* solution set $b_0 = \beta_0, b_1 = \beta_1$ that minimizes MSE. 

:::{.rmdnote data-latex="{tip}"}

__Technical Note__:

|               Technically (almost to the point of pedantry), a nonlinear model is a model where the first derivative of the expectation taken with respect to the parameters is itself a function of other parameters [@Seber1989]. For example,

$$ E(Y \mid X) = \beta_0 + \frac{X}{\beta_1}   $$
is a nonlinear model, because it's first derivative taken with respect to $\beta_1$ is still a function of $\beta_1$:
$$ \frac{d E(Y \mid X)}{d \beta_1} = \frac{d \left (\beta_0 + \frac{X}{\beta_1} \right ) }{d \beta_1} =  - \frac{X}{\beta_1^2}$$

Why is this important? Solutions to these regression equations (which serve as our estimates), are obtained by finding where the slope of the tangent line of the parameters is zero. To do this, we need to set the first derivative of these regression equations to zero. But if there are still parameters in these first derivative equations, then there will not be a unique solution to the equation, and finding an estimate will require more complex approaches. This is the complication introduced by nonlinear models. 

On the other hand, curvilinear models are easy to find solutions for, since their first derivatives are not functions of parameters. For instance, for a quadratic model such as:
$$ E(Y \mid X) = \beta_0 + \beta_1 X + \beta_2 X^2   $$
The first derivatives taken with respect to each parameters turn out to be:
$$ \frac{d E(Y \mid X, C)}{d \beta_0} = 1$$
$$ \frac{d E(Y \mid X, C)}{d \beta_1} = X$$
$$ \frac{d E(Y \mid X, C)}{d \beta_2} = X^2$$

Thus, even though the regression function will not be a "straight line" on a plot, this model is still linear.

:::

Specifically, we can re-write the mean squared error as a function of $b_0 + b_1 X$ to be $MSE(b_0,b_1) = E[(Y - (b_0 + b_1 X))^2]$. We need not get into details here, but taking the partial derivatives of $MSE(b_0, b_1)$ with respect to $b_0$ and $b_1$ gives us the ordinary least squares estimator [@Rencher2000, @Shalizi2019].

There are some important points to note in how we formulated the problem of estimating $E(Y \mid X) = f(X)$ with a linear model:

- What we needed to invoke to make this work is that a linear approximation to $f(X)$ is "good enough" for our interest. We need the linear approximation so we can take derivatives of the MSE function without running into problems. These assumptions are usually referred to as "regularity" conditions [@Longford2008].

- We didn't explicitly state it, but on the basis of Figure 1, this approach makes most sense if $Y$ is continuous. If $Y$ is binary, categorical, or time-to-event, the rationale motivating this approach starts to break down to a degree. That is, while it is possible to fit an OLS estimator to a binary outcome data, not everyone agrees that this is a good idea, and caution is warranted when doing so.

- We *did not* need to invoke any any assumptions about homoscedasticity, or independent and identically distributed (iid) observations. If we were able to make these assumptions, then we obtain an estimator that is the "best linear unbiased estimator" [@Rencher2000]. 

- We *did not* need to invoke any distributional assumptions about $Y$ (or, more specifically, the conditional mean of $Y$). If we can assume Gaussian with constant variance, then we can equate the OLS estimator with the maximum likelihood estimator with a Gaussian distribution and identify link function.

Unfortunately, in the way we formulated it above, the set of linear models we can use to find a solution $f(X)$ that minimizes $MSE(f(X))$ is limited. For instance, in the case where $Y$ is binary, and $E(Y) = P(Y=1)$, using a linear model such as $b_0 + b_1 X$ can easily lead to problems, most notably that predicted probabilities lie outside of the bounds $[0, 1]$. 

We may elect to constrain $f(X)$ so that the predictions are forced to lie within $[0, 1]$. For instance, we could use the inverse of the logistic function and define $f(X) = \frac{1}{1 + \exp(-b_0 - b_1 X)}$. However, in this case the derivatives would no longer work out as simply as we'd need them too (see Technical Note). In effect, we would now have to deal with the fact that the model is nonlinear (i.e., the derivatives of the model cannot simply be set to zero).^[I am taking many liberties here. The formulation under a logistic link function is different in that it is usually (required?) expressed in terms of likelihood functions, and not simply MSE. There are relations between these formulations, but we need not get into that here.] 

As it turns out, in the early 1970's, Nelder and Wedderburn [@Nelder1972] made a seminal contribution that enabled fitting nonlinear models when the outcome belongs to the exponential family of distributions,^[The exponential family of distributions is not to be confused with the exponential distribution. It refers to a family of distributions that can be re-written such that they can be represented in a common form. These distributions include (but are not limited to) the Gaussian, exponential, bernoulli (binomial), Poisson, and negative binomial.] and the conditional mean of the outcome can be linked to the covariates through some smooth and invertible linearizing function (which includes the $\log$ and $\logit$ functions).

# Generalized Linear Models

Generalized linear models consist of a family of regression models that are fully characterized by a selected distribution and a link function. That is, to fully specify a GLM, one must select a distribution (which determines the form of the conditional mean and variance of the outcome) and a link function (which determines how the conditional mean of the outcome relates to the covariates).

There are a wide variety of distributions and link functions available in standard statistical software programs that fit GLMs. Here, we'll consider a binary outcome $Y$ with probability $P(Y=1)$, and focus attention on three link functions: 

1. Logit, or the log-odds: $\log{P(Y=1)/[1-P(Y=1)]}$
2. Log: $\log[P(Y=1)]$
3. Identity: $P(Y=1)$.

A common misconception is that to use GLMs correctly, one must choose the distribution that best characterizes the data, as well as the canonical link function corresponding to this distribution. For example, if the outcome is binary, one "must" choose the binomial distribution with the logit link. While the binomial distribution and logit link work well together for binary outcomes, they do not easily provide contrasts like the risk difference or risk ratio, because of the selected link function. Alternative specification of the distribution and link function for GLMs can address this limitation.

## Link Functions and Effect Measures

There is an important relation between the chosen link function, and the interpretation of the coefficients from a GLM. For models of a binary outcome and the logit or log link, this relation stems from the properties and rules governing the natural logarithm. The quotient rule states that $\log(X/Y) = \log(X) − \log(Y)$.

Because of this relation, the natural exponent of the coefficient in a logistic regression model yields an estimate of the odds ratio. However, by the same reasoning, exponentiating the coefficient from a GLM with a log link function and a binomial distribution (i.e., log-binomial regression) yields an estimate of the risk ratio. Alternately, for GLM models with a binomial distribution and identity link function, because logarithms are not used, the unexponentiated coefficient yields an estimate of the risk difference.



Unfortunately, using a binomial distribution can lead to convergence problems with the $\log()$ or identity link functions for reasons that have been explored [@Zou2004]. This will occur when, for example, the combined numerical value of all the independent variables in the model is very large. This can result in estimated probabilities that exceed 1, which violates the very definition of a probability (binomial) model (probabilities can only lie between zero and one) and hence, convergence problems. Let's see how these problems can be overcome.

## A Data Example

We use data from the National Health and Nutrition Examination Survey (NHEFS), available as a companion dataset to the @Hernan2020 book.  We are interested primarily in the covariate adjusted association (on the risk difference and risk ratio scales) between quitting smoking and a greater than median weight change between 1971 and 1982.

In our analyses, we regress an indicator of greater than median weight change against an indicator of whether the person quit smoking. We adjust for exercise status, sex, age, race, income, marital status, education, and indicators of whether the person was asthmatic or had bronchitis. We start by loading the data:

```{r, warning = F, message = F}
#' Load relevant packages
packages <- c("broom","here","tidyverse","skimr","rlang","sandwich","boot", "kableExtra")

for (package in packages) {
  if (!require(package, character.only=T, quietly=T)) {
    install.packages(package, repos='http://lib.stat.cmu.edu/R/CRAN')
  }
}

for (package in packages) {
  library(package, character.only=T)
}

#' Define where the data are
file_loc <- url("https://cdn1.sph.harvard.edu/wp-content/uploads/sites/1268/1268/20/nhefs.csv")

#' This begins the process of cleaning and formatting the data
nhefs <- read_csv(file_loc) %>% 
  select(qsmk,wt82_71,wt82, wt71, exercise,sex,age,
         race,income, marital,school,
         asthma,bronch, 
         starts_with("alcohol"),-alcoholpy,
         starts_with("price"),
         starts_with("tax"), 
         starts_with("smoke"),
         smkintensity82_71) %>% 
  mutate(income=as.numeric(income>15),
         marital=as.numeric(marital>2),
         alcoholfreq=as.numeric(alcoholfreq>1)) %>% 
  na.omit(.)

factor_names <- c("exercise","income","marital",
                  "sex","race","asthma","bronch")
nhefs[,factor_names] <- lapply(nhefs[,factor_names] , factor)

#' Define outcome
nhefs <- nhefs %>% mutate(id = row_number(), 
                          wt_delta = as.numeric(wt82_71>median(wt82_71)),
                          .before = qsmk)

#' Quick summary of data
nhefs %>% print(n=5)
```


## GLMs for risk differences and ratios

For our analyses of the data described above using GLM with a binomial distributed outcome with a log link function to estimate the risk ratio and identity link function to estimate risk difference, an error is returned:

```{r, error = T}
#' Here, we start fitting relevant regression models to the data.
#' modelForm is a regression argument that one can use to regress the 
#' outcome (wt_delta) against the exposure (qsmk) and selected confounders.

formulaVars <- paste(names(nhefs)[c(3,7:16)],collapse = "+")
modelForm <- as.formula(paste0("wt_delta ~", formulaVars))
modelForm

#' This model can be used to quantify a conditionally adjusted 
#' odds ratio with correct standard error
modelOR <- glm(modelForm,data=nhefs,family = binomial("logit"))
tidy(modelOR)[2,]

#' This model can be used to quantify a conditionally adjusted risk 
#' ratio with with correct standard error
#' However, error it returns an error and thus does not provide any results.
modelRR_binom <- glm(modelForm,data=nhefs,family = binomial("log"))
```

Why is this error returned? The most likely explanation in this context is as follows: We are modeling $P(Y = 1 \mid X) = \exp\{\beta_0 + \beta_1 X_1 + \ldots + \beta_p X_p\}$. In this context, there may be *no set of values* for the parameters in the model that yield $P(Y = 1 \mid X) < 1$ for every observation in the sample. Because R's glm function (under a binomial distribution) correctly recognizes this as a problem, it returns an error.

Instead, one may resort to using different distributions that are more compatible with the link functions that return the association measures of interest. For the risk ratio, one may use a GLM with a Poisson distribution and log link function. Doing so will return an exposure coefficient whose natural exponent can be interpreted as a risk ratio. 

```{r}
#' This model can be used to quantify a conditionally risk ratio 
#' using the Poisson distribuiton and log link function. 
#' However, because the Poisson distribution is used, the model 
#' provides incorrect standard error estimates.
modelRR <- glm(modelForm,data=nhefs,family = poisson("log"))
tidy(modelRR)[2,]
```

It's important to recognize what we're doing here. We are using this model as a tool to quantify the log mean ratio contrasting $P(Y = 1 \mid X_{qsmk} = 1)$ to $P(Y = 1 \mid X_{qsmk} = 0)$ (all other things being equal). However, we should not generally assume that ever aspect of this model is correct. In particular, note that the max predicted probability from this model is `r round(max(modelRR$fitted.values), 3)`:

```{r, message=F, warning=F}
summary(modelRR$fitted.values)
```

We can use the `augment` function in the `broom` package to evaluate the distribution of these probabilities (among other things):
```{r, message=F, warning=F}
fitted_dat <- augment(modelRR, type.predict="response")

fitted_dat

plot_hist <- ggplot(fitted_dat) +
  geom_histogram(aes(.fitted)) +
  scale_y_continuous(expand=c(0,0)) +
  scale_x_continuous(expand=c(0,0))

ggsave(here("figures","2022_02_21-rr_hist_plot.pdf"), plot=plot_hist)
```

This distribution is shown in margin Figure \ref{fig:fittedhist}. We can also see that there are only two observations in the sample with predicted risks greater than 1.

```{r fittedhist, out.width="5cm", fig.align='center', fig.margin=TRUE, echo=F, fig.cap="Distribution of fitted values from the Poisson GLM with log link function to obtain an estimate of the adjusted risk ratio for the association between quitting smoking and greater than median weight gain in the NHEFS."}
knitr::include_graphics(here("figures","2022_02_21-rr_hist_plot.pdf"))
```

```{r}
fitted_dat %>% 
  filter(.fitted >= 1) %>% 
  select(wt_delta, qsmk, age, .fitted)
```

For these reasons, we are not particularly concerned about the fact that the model predicts risks that are slightly large than 1. However, the model-based standard errors (i.e., the SEs that one typically obtains directly from the GLM output) are no longer valid. Instead, one should use the robust (or sandwich) variance estimator to obtain valid SEs (the bootstrap can also be used) [@Zou2004].

```{r}
#' To obtain the correct variance, we use the "sandwich" 
#' function to obtain correct sandwich (robust) standard 
#' error estimates.
sqrt(sandwich(modelRR)[2,2])
```

For the risk difference, one may use a GLM with a Gaussian (i.e., normal) distribution and identity link function, or, equivalently, an ordinary least squares estimator. Doing so will return an exposure coefficient that can be interpreted as a risk difference. However, once again the robust variance estimator (or bootstrap) should be used to obtain valid SEs.

```{r}
#' This model can be used to obtain a risk difference 
#' with the gaussian distribiton or using ordinary least 
#' squares (OLS, via the lm function). Again, the model 
#' based standard error estimates are incorrect. 
modelRD <- glm(modelForm,data=nhefs,family = gaussian("identity"))
modelRD <- lm(modelForm,data=nhefs)
tidy(modelRD)[2,]
#' To obtain the correct variance, we use the "sandwich" function
#' to obtain correct sandwich (robust) standard error estimates.
sqrt(sandwich(modelRD)[2,2])
```

The risk ratio and difference, as well as the 95% sandwich variance confidence intervals, obtained for the relation between quitting smoking and greater than median weight change are provided Table 1.

```{r, echo = F}
table1_data <- tibble(Method = c("GLM","Marginal Standardization"),
                      `Risk Difference` = c("0.14	(0.09, 0.20)", "0.14	(0.09, 0.21)"), 
                      `Risk Ratio` = c("1.32	(1.19, 1.46)", "1.31	(1.18, 1.46)"))
```

```{r}
knitr::kable(table1_data)
```

Results in this table obtained using a conditionally adjusted regression model without interactions. Gaussian distribution and identity link was used to obtain the risk difference. A Poisson distribution and log link was used to obtain the risk ratio. 95% CIs obtained via the sandwich variance estimator. 95% CIs obtained using the bias-corrected and accelerated bootstrap CI estimator.

Unfortunately, use of a Poisson or Gaussian distribution for GLMs for a binomial outcome can introduce different problems. For one, while not entirely worrysome in our setting, a model that predicts probabilities greater than one should not instill confidence in the user. Second, performance of the robust variance estimator is notoriously poor with small sample sizes. Finally, the interpretation of the risk differences and ratios becomes more complex when the exposure interacts with other variables in the model. 

```{r, echo = F}
table2_data <- tibble(`Odds Ratio` = c("GLM Family = Binomial", "GLM Link = Logistic", "Standard Errors = Model Based", " ", " ", " ", " ", " "), 
                      `Risk Ratio` = c("GLM Family = Binomial", "GLM Link = Log", "Standard Errors = Model Based", "GLM Family = Poisson", "GLM Link = Log", "Standard Errors = Sandwich", " ", " "),
                      `Risk Difference` = c("GLM Family = Binomial", "GLM Link = Identity", "Standard Errors = Model Based", "GLM Family = Gaussian", "GLM Link = Identity", "Standard Errors = Sandwich", "Least Squares Regression", "Standard Errors = Sandwich"))
```

```{r, echo=F}
knitr::kable(table2_data, "simple", caption = "Methods to use for quantifying conditionally adjusted odds ratios, risk ratios, and risk differences.")
```

For instance, let's assume that in the NHEFS data, the association between quitting smoking and weight gain interacts with baseline exercise status:

```{r}
#' Potential evidence for interaction 
#' between smoking and exercise on the risk difference scale?

table(nhefs$exercise)

names(nhefs)[c(3,7:16)]

formulaVars <- paste(names(nhefs)[c(3,7:16)],collapse = "+")

modelForm <- as.formula(paste0("wt_delta ~", formulaVars))
modelForm

modelForm_int <- as.formula(paste0("wt_delta ~", formulaVars,"+ qsmk*exercise"))
modelForm_int

summary(glm(modelForm,data=nhefs, family=binomial(link="identity")))
summary(glm(modelForm_int,data=nhefs, family=binomial(link="identity")))
```


If this were the case, to properly interpret the association, the interaction between exercise status and qsmk should be considered. But how could we do this? One approach would be to include the interaction term and interpret the association between quitting smoking and weight change separately for each level of exercise.

For example, in the model that includes the interaction term with exercise, we can no longer simply interpret the coefficient for `qsmk` as the treatment effet of interest. Instead, (under causal identifiability) we have three treatment effects: The effect of `qsmk` for those with `exercise = 0, 1,` and `2`. If we were, in fact, interested in the average treatment effect in the sample (and not a unique treatment effect for each level of exercise), we would have to take a weighted average of the coefficients for these effects, where the weights are defined as a function of the proportion of individuals in each exercise level. 

Clearly, this approach can quickly become too burdensome when there are several relevant interactions in the model, and is not worth the effort when we are interested in the marginal association. As an alternative, we can use marginal or model-based standardization, which can greatly simplify the process.

## Marginal or Model-Based Standardization

Another approach to obtaining risk differences and ratios from GLMs that are not subject to the limitations noted above is to use marginal standardization, which is equivalent to g computation when the exposure is measured at a single time point [@Naimi2016b]. This process can be implemented by fitting a single logistic model, regressing the binary outcome against all confounder variables, including all relevant interactions. But instead of reading the coefficients the model, one can obtain odds ratios, risk ratios, or risk differences by using this model to generate predicted risks for each individual under “exposed” and “unexposed” scenarios in the dataset. To obtain standard errors, the entire procedure must be bootstrapped (see supplemental material for code). These marginal risk differences and ratios, as well as their bootstrapped CIs are presented in the table above.

Here is some code to implement this marginal standardization in the NHEFS data:
```{r}
#' Marginal Standardization: version 1
formulaVars <- paste(names(nhefs)[c(3,7:16)],collapse = "+")
modelForm <- as.formula(paste0("wt_delta ~", formulaVars,"+ qsmk*exercise")) ## include the interaction!
modelForm

#' Regress the outcome against the confounders with interaction
ms_model <- glm(modelForm,data=nhefs,family=binomial("logit"))
##' Generate predictions for everyone in the sample to obtain 
##' unexposed (mu0 predictions) and exposed (mu1 predictions) risks.
mu1 <- predict(ms_model,newdata=transform(nhefs,qsmk=1),type="response")
mu0 <- predict(ms_model,newdata=transform(nhefs,qsmk=0),type="response")

#' Marginally adjusted odds ratio
marg_stand_OR <- (mean(mu1)/mean(1-mu1))/(mean(mu0)/mean(1-mu0))
#' Marginally adjusted risk ratio
marg_stand_RR <- mean(mu1)/mean(mu0)
#' Marginally adjusted risk difference
marg_stand_RD <- mean(mu1)-mean(mu0)

#' Using the bootstrap to obtain confidence intervals for the marginally adjusted 
#' risk ratio and risk difference.
bootfunc <- function(data,index){
  boot_dat <- data[index,]
  ms_model <- glm(modelForm,data=boot_dat,family=binomial("logit"))
  mu1 <- predict(ms_model,newdata=transform(boot_dat,qsmk=1),type="response")
  mu0 <- predict(ms_model,newdata=transform(boot_dat,qsmk=0),type="response")
  
  marg_stand_OR_ <- (mean(mu1)/mean(1-mu1))/(mean(mu0)/mean(1-mu0))
  marg_stand_RR_ <- mean(mu1)/mean(mu0)
  marg_stand_RD_ <- mean(mu1)-mean(mu0)
  res <- c(marg_stand_RD_,marg_stand_RR_,marg_stand_OR_)
  return(res)
}

#' Run the boot function. Set a seed to obtain reproducibility
set.seed(123)
boot_res <- boot(nhefs,bootfunc,R=2000)

boot_RD <- boot.ci(boot_res,index=1)
boot_RR <- boot.ci(boot_res,index=2)
boot_OR <- boot.ci(boot_res,index=3)

marg_stand_OR
marg_stand_RR
marg_stand_RD

boot_RD
boot_RR
boot_OR

```

While this marginal standardization approach is more flexible in that it accounts for the interaction between quitting smoking and exercise, and still yields an estimate of the average treatment effect (again, under identifiability), it still assumes a constant effect of qsmk on weight change across levels of all of the other variables in the model. This constant effect assumption might be true, but if one wanted to account for potential interactions between the exposure and all of the confounders in the model, there is an easy way. We call this the "stratified modeling approach."

This stratified modeling approach avoids the exposure effect homogeneity assumption across levels of all the confounders. In effect, the approach fits a separate model for each exposure stratum. To obtain predictions under the “exposed” scenario, we use the model fit to the exposed individuals to generate predicted outcomes in the entire sample. To obtain predictions under the “unexposed” scenario, we repeat the same procedure, but with the model fit among the unexposed. One can then average the risks obtained under each exposure scenario, and take their difference and ratio to obtain the risk differences and ratios of interest.
```{r}
#' Marginal Standardization
##' To avoid assuming no interaction between 
##' smoking and any of the other variables
##' in the model, we subset modeling among 
##' exposed/unexposed. This code removes smoking from the model,
##' which will allow us to regress the outcome 
##' against the confounders among the exposed and 
##' the unexposed searately. Doing so will allow us 
##' to account for any potential exposure-covariate interactions
##' that may be present. 
formulaVars <- paste(names(nhefs)[c(7:16)],collapse = "+")
modelForm <- as.formula(paste0("wt_delta ~", formulaVars))
modelForm

#' Regress the outcome against the confounders 
#' among the unexposed (model0) and then among the exposed (model1)
model0 <- glm(modelForm,data=subset(nhefs,qsmk==0),family=binomial("logit"))
model1 <- glm(modelForm,data=subset(nhefs,qsmk==1),family=binomial("logit"))
##' Generate predictions for everyone in the sample using the model fit to only the 
##' unexposed (mu0 predictions) and only the exposed (mu1 predictions).
mu1 <- predict(model1,newdata=nhefs,type="response")
mu0 <- predict(model0,newdata=nhefs,type="response")

#' Marginally adjusted odds ratio
marg_stand_OR <- (mean(mu1)/mean(1-mu1))/(mean(mu0)/mean(1-mu0))
#' Marginally adjusted risk ratio
marg_stand_RR <- mean(mu1)/mean(mu0)
#' Marginally adjusted risk difference
marg_stand_RD <- mean(mu1)-mean(mu0)

#' Using the bootstrap to obtain confidence intervals for the marginally adjusted 
#' risk ratio and risk difference.
bootfunc <- function(data,index){
  boot_dat <- data[index,]
  model0 <- glm(modelForm,data=subset(boot_dat,qsmk==0),family=binomial("logit"))
  model1 <- glm(modelForm,data=subset(boot_dat,qsmk==1),family=binomial("logit"))
  mu1 <- predict(model1,newdata=boot_dat,type="response")
  mu0 <- predict(model0,newdata=boot_dat,type="response")
  
  marg_stand_OR_ <- (mean(mu1)/mean(1-mu1))/(mean(mu0)/mean(1-mu0))
  marg_stand_RR_ <- mean(mu1)/mean(mu0)
  marg_stand_RD_ <- mean(mu1)-mean(mu0)
  res <- c(marg_stand_RD_,marg_stand_RR_,marg_stand_OR_)
  return(res)
}

#' Run the boot function. Set a seed to obtain reproducibility
set.seed(123)
boot_res <- boot(nhefs,bootfunc,R=2000)

boot_RD <- boot.ci(boot_res,index=1)
boot_RR <- boot.ci(boot_res,index=2)
boot_OR <- boot.ci(boot_res,index=2)

marg_stand_OR
marg_stand_RR
marg_stand_RD

boot_RD
boot_RR
boot_OR

```


When predicted risks are estimated using a logistic model, relying on marginal standardization will not result in probability estimates outside the bounds [0, 1]. And because the robust variance estimator is not required, model-based standardization will not be as affected by small sample sizes. However, the bootstrap is more computationally demanding than alternative variance estimators, which may pose problems in larger datasets.

# References