---
title: "Regression in Time-Fixed Settings: Part 3"
author: "Ashley I Naimi"
date: "Spring 2022"
urlcolor: blue
bibliography: ref.bib
link-citations: yes
output: 
    bookdown::pdf_book:
      base_format: tint::tintPdf
      toc: true
      number_sections: true
      includes:
        in_header: "../misc/preamble.tex"
      latex_engine: xelatex
    html_document:
      theme: readable
      toc: true
      toc_float: true
      number_sections: true
      css: "../misc/style.css"
---

```{r setup, include=FALSE}
library(knitr)
opts_chunk$set(tidy.opts=list(width.cutoff=40),tidy=TRUE)

packages <- c( "data.table","tidyverse","ggplot2","ggExtra","formatR",
               "gridExtra","skimr","here","RColorBrewer", "broom")

for (package in packages) {
  if (!require(package, character.only=T, quietly=T)) {
    install.packages(package, repos='http://lib.stat.cmu.edu/R/CRAN',dependencies=T)
  }
}

for (package in packages) {
  library(package, character.only=T)
}

remotes::install_github("rstudio/fontawesome")

library(fontawesome)

thm <- theme_classic() +
  theme(
    legend.position = "top",
    legend.background = element_rect(fill = "transparent", colour = NA),
    legend.key = element_rect(fill = "transparent", colour = NA)
  )
theme_set(thm)
```

\newpage
\onehalfspacing

\newpage
\onehalfspacing

# Introduction to Propensity Score Methods

So far in this course, we've focused on obtaining quantitative estimates of the average treatment effect using an outcome modeling approach. In this approach, one regresses the outcome against the exposure and confounders. This works for the conditionally adjusted estimator, where we read the coefficient for the exposure from the regression model, or the marginally adjusted estimator (e.g., using marginal standardization). The basic formulation of this outcome modeling approach can be depicted heuristically using the DAG in Figure \ref{fig:figure1}:

```{r figure1, out.width="10cm", fig.align='center', fig.margin=FALSE, echo=F, fig.cap="Directed acyclic graph depicting confounder adjustment using an outcome modelling approach. In this approach, information from the confounder to the outcome is 'blocked' (blue arrow). With this adjustment, the exposure $X$ os $d$-separated from the outcome $Y$, rendering the estimate of the exposure-outcome association unconfounded."}
knitr::include_graphics(here("figures","2022_03_21-Section3_Figure1.pdf"))
```

In this Figure, the open back-door path from $X$ to $Y$ is blocked by conditioning on $C$. This leads to a conditionally adjusted estimate of the exposure effect. One can marginalize over the distribution on $C$ to get a marginally adjusted estimate from a model for the outcome. For a binary outcome, such estimates may be obtained on the risk difference, risk ratio, or odds ratio scales, or one may obtain a marginally adjusted estimate of the cumulative risk function that would be observed if everyone were exposed or unexposed.  

On the other hand, we may want to obtain an adjusted estimate of the exposure effect by modeling the exposure. This can be accomplished using propensity score methods. 

The propensity score was first defined by Rosenbaum and Rubin [@Rosenbaum1983] as the conditional probability of receiving the treatment (or, equivalently, of being exposed). The true propensity score is defined as 

$$e(C) = P(X = 1 \mid C)$$
This propensity score is typically known in a randomized trial. It's important to distinguish this true PS from the estimated PS:

$$\hat{e}(C) =  \hat{P}(X = 1 \mid C)$$
This estimated PS can be fit using a parametric model, in which case, we might write: 
$$\hat{e}(C; \alpha) =  \hat{P}(X = 1 \mid C) = \expit ( \alpha C)$$
For reasons that are not entirely obvious, it is usually better to use the estimated PS, even when the true PS is known [@Robins1992c, @Henmi2004].

If the set of conditioning variables consists of the relevant confounding variables, the propensity score can be used to invoke conditional exchangeability. To see why, consider the case where the probability of being exposed is conditional on two binary confounders:

$$ f(C) = P(X = 1 \mid C) = \expit(\alpha_0 + \alpha_1 C_1 + \alpha_2 C_2 + \alpha_3 C_1 C_2)$$

Consider that, for two binary confounders, there are four possible joint confounder levels:

```{r echo = F, results = 'asis'}
library(knitr)
conf <- tibble(C1 = c(0,1,0,1), 
               C2=c(0,0,1,1))
kable(conf, "pipe")
```

Notice also that there are four parameters in the above model, one parameter for each level. This implies that, for this simple example, there is a unique propensity score value for each unique confounder level. In other words, the one-dimensional propensity score in this example contains all the information available in the two-dimensional set of confounders. We can thus reduce the complexity of the set of confounders to a single variable, and adjust for this variable instead of the confounders. For example:

$$E[Y \mid X, p(X)] = \beta_0 + \beta_1 X + \beta_c f(C)$$
Heuristically (again), we can show why this approach works using the DAG in Figure 2

```{r figure2, out.width="10cm", fig.align='center', fig.margin=FALSE, echo=F, fig.cap="Directed acyclic graph depicting confounder adjustment using an outcome modelling approach. In this approach, information from the confounder to the outcome is 'blocked' (blue arrow). With this adjustment, the exposure $X$ os $d$-separated from the outcome $Y$, rendering the estimate of the exposure-outcome association unconfounded."}
knitr::include_graphics(here("figures","2022_03_21-Section3_Figure2.pdf"))
```

In this DAG, $f(C)$ is a proxy for all the variables $C$. Thus, we can use $f(C)$ to replace $C$ in an outcome regression model.

As a univariate proxy for the multivariate set of confounders, we can use the propensity score to adjust for confounding using a number of different techniques. These techniques include regression adjustment, propensity score matching, stratification, and weighting. In this lecture, we will quickly cover the first three and focus on the creation and use of IP-weights. The reason for this emphasis is that $i$ in epidemiology, the most commonly used PS technique is inverse probability weighting, and so it is important to know how to implement; and $ii$, there are theoretical justifications suggesting that weighting is the most optimal use of the PS. 

# Adjustment, Matching, Stratification

To demonstrate these techniques, we will again use the Section 1 cohort data:

```{r, warning=F, message=F}
a <- read_csv(here("data","2022_03_09-Section1_cohort.csv")) %>% 
  mutate(outcome_one = as.numeric(outcome==1))

print(a, n=5)
```

The first step in any PS analysis is to obtain an estimate of the propensity score. This can be done using an array of techniques, including nonparametric techniques such as machine learning methods [e.g., @Lee2010] or a more general nonparametric approach [@Hirano2003] (see Technical Note), but is most often accomplished using logistic regression. 

:::{.rmdnote data-latex="{tip}"}

__Technical Note__:

|               Using the propensity score to adjust for confounding is a technique that falls into the class of single robust estimation. Singly robust estimators rely on a single model to adjust for confounding (or missing data or selection bias). In this case, the single model is the propensity score model. 

Many researchers have proposed or implemented single robust estimation methods using machine learning methods. Machine learning methods consist of a wide range of analytic techniques that do not require hard to verify modeling assumptions. Because of this, they are often assumed to be less biased than their standard parametric counterparts. This perceived property has motivated many to either recommended or use machine learning methods to quantify exposure effects [@Lee2010,@Westreich2010c,@Snowden2011,@Oulhote2019]. These ``machine learning'' methods include techniques like kernel regression, splines, random forests, boosting, etc., which exploit smoothness across covariate patterns to estimate the regression function. 

However, for any nonparametric approach there is an explicit bias-variance trade-off that arises in the choice of tuning parameters; less smoothing yields smaller bias but larger variance, while more smoothing yields smaller variance but larger bias (parametric models can be viewed as an extreme form of smoothing). This tradeoff has important consequences. 

Convergence rates for nonparametric estimators become slower with more flexibility and more covariates. For example, a standard rate for estimating smooth regression functions is $N^{-\beta/(2\beta+d)}$, where $\beta$ represents the number of derivatives of the true regression function, and $d$ represents the dimension of, or number of covariates in, the true regression function. This issue is known as the **curse of dimensionality** [@Gyorfi2002,@Robins1997c,@Wasserman2006]. Sometimes this is viewed as a disadvantage of nonparametric methods; however, it is just the cost of making weaker assumptions: if a parametric model is misspecified, it will converge very quickly to the wrong answer. 

In addition to slower convergence rates, confidence intervals are harder to obtain. Specifically, even in the rare case where one can derive asymptotic distributions for nonparametric estimators, it is typically not possible to construct confidence intervals (even via the bootstrap) without impractically undersmoothing the regression function (i.e., overfitting the data) [@Wasserman2006]. 

These complications (slow rates and lack of valid confidence intervals) are generally inherited by singly robust estimators such as methods that use only the propensity score for adjustment (this is apart from a few special cases which require simple estimators, such as kernel methods with strong smoothness assumptions and careful tuning parameter choices that are suboptimal for estimating $f$ or $g$). 

For general nonparametric estimators of the exposure or outcome model, convergence will be slow, and honest confidence intervals will not be computable (note that "honest" confidence intervals are defined as CIs with a minimum coverage probability no less than the nominal value over a rich class of nonparametric regression functions).

For these reasons, it is generally not advisable to use machine learning or nonparametric methods to estimate the propensity score and then proceed with PS adjustment in a regression model, matching, stratification, or weighting [@Naimi2022]. Instead, one should implement double robust estimation methods when machine learning or nonparametric methods are used.

:::

In a previous lecture, we discussed using different link functions and distributions to estimate a conditionally adjusted risk difference, risk ratio, or odds ratio. However, when using parametric regression to estimate the propensity score, one would typically (always?)^[Generally, you will always want to use a method that bounds the predicted probabilities between 0 and 1.] use logistic regression:

```{r, warning = F, message = F}

# create the propensity score in the dataset
a$propensity_score <- glm(exposure ~ confounder, data = a, family = binomial("logit"))$fitted.values

```

## Propensity Score Adjustment

For regression adjustment, we can then simply adjust for this propensity score in an outcome regression model to obtain an adjusted estimate of the parameter of interest. This is one of the easiest techniques available for using the propensity score to adjust for confounders:

```{r}

# adjust for the propensity score to obtain conditionally adjusted risk difference
model1 <- glm(outcome_one ~ exposure + propensity_score, data = a, family = binomial("identity"))

summary(model1)$coefficients

```

In the above output, the point estimate can be interpreted as the risk difference, with valid 95\% CIs (binomial distribution, identity link). In principle, nothing here prevents us from using a logistic link function, and then marginalizing over the distribution the way we do with marginal standardization and the confounders (though we must use the bootstrap for standard error assessment).

## Propensity Score Stratification

For propensity score stratification, we can create quantiles of the propensity score, and quantify the exposure effect within each stratum of this categorized propensity score. One can then combine the stratum specific point estimates and standard errors using a simple weighted average [@Lunceford2004]. In a simple setting with a single binary confounder, there will be only two unique propensity score values:

```{r}

table(a$propensity_score)

```

In this situation, we'd create two strata to implement PS stratification. Because there are only two PS levels, we can use this code to create the strata:

```{r}

a$ps_strata <- factor(a$propensity_score,labels=c("1","2"))

a %>% group_by(ps_strata) %>%
   count()

```

In more general settings with a propensity score that takes on many unique values (i.e., continuous between 0 and 1), we can use this code to construct and evaluate quantiles:

```{r, eval = F}

## Identity the quintiles of the PS
quants <- quantile(a$propensity_score, prob = seq(0,1,by=1), na.rm = T)

print(quants)

a$ps_strata <- cut(a$propensity_score, breaks = quants, include.lowest = T)

a %>% group_by(ps_strata) %>%
   count()

```

Either way, once the PS strata are created, the next step is to estimate the stratum specific associations of interest. Here, we quantify the risk differences using the binomial distribution and identity link:

```{r}
# Perform logistic regression within each stratum

stratified_ps <- a %>% 
  group_by(ps_strata) %>% 
  do(model = glm(outcome_one ~ exposure, data=., family=binomial("identity")))

stratified_ps$model[[1]]$coefficients

stratified_ps$model[[2]]$coefficients
```

The stratum specific coefficients and standard errors can then be combined using standard equations:

$$\hat{\gamma} = \frac{1}{K}\sum_{k=1}^K \bigg \{ \hat{\gamma}^{(i)} \bigg \}, \;\;\;\;\;\; SE(\hat{\gamma}) = \sqrt{\frac{1}{K^2}\sum_{k=1}^K \bigg \{ SE(\hat{\gamma}^{(i)})^2 \bigg \}}$$
## Propensity Score Matching

Finally, one can match on the basis of the propensity score, using a range of different techniques. These include 1:1 or 1:M matching, matching with or without replacement, greedy versus optimal matching, and nearest neighbor versus caliper matching [@Austin2011].

```{r}
library(MatchIt)
library(optmatch)

ps_matching <- matchit(formula = exposure ~ confounder, data     = a,
                       method = "full", distance = "logit", # Distance defined by usual propensity score from logistic model
                       ratio = 1, # gives us 1:1 matching, which is the default
                       estimand = "ATE"
                       )

ps_matching

```

Once we've identified the matched pairs, we can create a dataset that contains the information we need to match. The `matchit` function in `R`, as with some other matching approaches, constructs a set of weights to operationalize the matched sets [@Yoshida2017]. The dataset will include a weights variable and subclass variable that provide the information needed to conduct the matched analysis:

```{r}

matched_data <- match.data(ps_matching)

matched_data %>% select(ID, exposure, confounder,
                        propensity_score, distance, 
                        weights, subclass) %>% 
  print(n=6)

```

One can then use this dataset with a standard `glm` model, weighted to implement the matching:

```{r}

matched_model <- glm(outcome_one ~ exposure, 
                     data = matched_data, 
                     weights = weights,
                     family = binomial(link = "logit"))

summary(matched_model)$coefficients

```

This model creates a warning referring to "non-integer #successes in a binomial glm". This warning is referring to the fact that, while each individual contributes either 0 or 1 event to the likelihood function, with the weights added, these "contributions" become non-integer values. For example, if an individual's weight is 0.718, they will contribute a total of 0.718 events to the overall analysis. This is not actually a problem when we use weights to conduct an analysis. It's just the consequence of using weights. Thus, the only real problem is the warning that we get (not the content of the warning itself).

One "solution" to this artificial problem^[This "artificial" problem becomes real in the context of, e.g., a simulation study where the warning will prematurely interrupt the simulation. That is, sometimes a warning will interrupt a function that needs to be run in full. In this case, it's helpful to use the quasibinomial approach, which is identical to the binomial except no warning is thrown.] is to use the `quasibinomial` distribution instead of the `binomial` option.

The coefficient from this model can be interpreted as an average treatment effect obtained via matching. The next step is to obtain a standard error for this parameter. This is where things get a little uncertain. There are generally two approaches used to obtain standard errors for a matching estimator: the robust (sandwich) variance estimator, and the bootstrap. 

Here is some code to implement the robust variance estimator. Note the need for the "subclass" argument and the ID argument:

```{r, warning = F, message = F}

library(lmtest)
library(sandwich)

coeftest(matched_model, vcov. = vcovCL, cluster = ~ subclass + ID)

```

The problem here is that there is comparatively little evidence supporting the use of the robust (sandwich) variance estimator or the bootstrap for matching estimators. There is even some theoretical work suggesting that the bootstrap is biased for a propensity score matching estimator [@Abadie2008]. 

# Inverse Probability Weighting: Intuition

The most commonly employed propensity score adjustment technique is inverse probability weighting. The simple heuristic often used to describe the way IP-weighting works is that, when applied to data, they yield a "pseudo population" where there is no longer an effect of the confounder on the exposure. The causal structure of the variables in this new pseudo-population can be depicted in Figure \ref{fig:figure3}:

```{r figure3, out.width="10cm", fig.align='center', fig.margin=FALSE, echo=F, fig.cap="Directed acyclic graph depicting the causal relations between variables in a 'pseudo-population' obtained via inverse probability weighting. Again, with this adjustment, the exposure $X$ os $d$-separated from the outcome $Y$, rendering the estimate of the exposure-outcome association unconfounded."}
knitr::include_graphics(here("figures","2022_03_21-Section3_Figure3.pdf"))
```

The weights for each individual needed to create this pseudo-population are defined as the inverse of the probability of receiving their observed exposure. Let's consider the following simple example to explain why this works. In this example, there are 20 observations, with one binary confounder (50:50 split) and one binary exposure. Let's suppose the probability that the exposure `x = 1` is 0.2 for those with `c = 0` and 0.9 for those with `c = 1`:

Under these scenarios, we'd have exposure and confounder data that looks like this:

```{r}

c <- c(0,0,1,1)
x <- c(1,0,1,0)
n <- c(2,8,9,1)

tibble(x,c,n)

```

Let's focus on the stratum of individuals with `c = 0`. In this stratum, there are 2 exposed individuals, and 8 unexposed individuals. Now let's ask what these data should look like if we were able to implement an ideal (marginally) randomized trial with the probability of treatment being 50% for all individuals. We would expect that within the stratum of individuals with `c = 0`, there would be 5 exposed and 5 unexposed individuals. Inverse probability weighting seeks to accomplish this balance. 

Consider that the inverse probability of the **observed exposure** among those with `c = 0` is 0.2 for those who were actually exposed, and 0.8 for those who were unexposed. 

The inverse probability weight is thus $\frac{1}{0.2} = 5$ for the exposed in this confounder stratum, and $\frac{1}{0.8} = 1.25$ for the unexposed in this confounder stratum.

This suggests that, in their contribution to the overall analysis, the two exposed individuals in the `c = 0` status would each receive a weight of 5, while the eight unexposed individuals in the `c = 0` stratum would each receive a weight of 1.25. Under these conditions, we would have a re-balanced set of observations in the `c = 0` stratum of:

$$\text{Exposed Observations:}\;\;\;\; 5 \times 2 = 10$$
$$\text{Unexposed Observations:}\;\;\;\; 8 \times 1.25 = 10$$
In effect, our inverse probability weighting strategy made it such that we now have an equal number of exposed and unexposed observations within the `c = 0` stratum. In these weighted data, we can now compute the difference in the outcome among the exposed and unexposed individuals (if we had it) to obtain an estimate of our average treatment effect.

# Inverse Probability Weighting In Practice

Simply taking the inverse of the probability of the observed exposure, while valid, is not the usual strategy for implementing inverse probability weights. In practice, one will often use stabilized weights, stabilized normalized weights, potentially with some degree of "truncation" or, more accurately, trimming of the weights.^[In contrast to our emphasis of the usage of the word "truncation" which refers to the removal of observations from the dataset, researchers will often refer to "truncating" the weights, which sets the largest value to be equal to the 99th or 95th percentile values. This is more accurately referred to as "trimming" the weights, since no truncation is occurring.] 

Furthermore, it's important to note that "data" are often not weighted, but rather the contribution that each individual in the sample makes to the estimating function (e.g., likelihood, estimating equation, or other function used to find parameters). This is important in that one must choose a fitting algorithms that allows for this type of weighting.

To start, the simplest type of weight used in practice is the stabilzied inverse probability weight. These are often defined as:

\[
sw = 
\begin{dcases}
\frac{P(X = 1)}{P(X = 1 \mid C)} & \text{if $X = 1$} \\
\frac{P(X = 0)}{P(X = 0 \mid C)} & \text{if $X = 0$}
\end{dcases}
\]

but are sometimes written more succinctly as^[This formulation is unusual, since $f(.)$ represents the probability density function, which is usually taken for a specific realization of the random variable. However, in this case, because the weights are defined as a function of the observed exposure status, the argument in the operator is the observed data (denoted with capital letter), as opposed to some specific realization.]:

$$sw = \frac{f(X)}{f(X \mid C)}$$
Let's use the cohort data again to construct the stabilized weights. We will re-fit the PS model to the data, and construct the weights:

```{r, warning = F, message = F}

# create the propensity score in the dataset
a$propensity_score <- glm(exposure ~ confounder, data = a, family = binomial("logit"))$fitted.values

# stabilized inverse probability weights
a$sw <- (mean(a$exposure)/a$propensity_score)*a$exposure + 
  ((1-mean(a$exposure))/(1-a$propensity_score))*(1-a$exposure)

summary(a$sw)

a %>% select(ID, exposure, confounder, outcome_one, propensity_score, sw) %>% print(n = 5)

```

As we can see from the output above, the stabilized weights are, in fact, well behaved, with a mean of one and a max value that is small.

```{r}

model_RD_weighted <- glm(outcome_one ~ exposure, data = a, weights=sw, family = quasibinomial("identity"))

summary(model_RD_weighted)$coefficients

```

At times, the mean and max of the stabilized weights are sub-optimal, in that the mean may not be one, or the max may be too large for comfort. One strategy we can use here is to normalize the weights, by dividing the stabilized weights by the max stabilized weight:

```{r}

a$sw_norm <- a$sw/max(a$sw)

summary(a$sw_norm)

a %>% select(ID, exposure, confounder, outcome_one, propensity_score, sw, sw_norm) %>% print(n = 5)

```

In this case, the mean of the normalized weights is no longer expected to be one. However, the max weight will be one by definition. These weights can then be used in the same way:

```{r}

model_RD_weighted_norm <- glm(outcome_one ~ exposure, data = a, weights=sw_norm, family = quasibinomial("identity"))

summary(model_RD_weighted_norm)$coefficients

```

Finally, instead of normalizing, sometimes researchers will "trim" the weights to avoid problems induced by very large weights. The procedure for doing this is straightfoward, and requires simply replacing all weight values greater than a certain percentile with their percentile values:

```{r}

quantile(a$sw, .8)

a <- a %>% mutate(sw_trim = if_else(sw>quantile(sw, .80),
                                    quantile(sw, .80),
                                    sw))

```

We can see how this changes the values in the following plot:

```{r}

ggplot(a) + geom_jitter(aes(sw, sw_trim))

```

In this example analysis, we use the 80th percentile of the distribution of the stabilized weights to trim. In more typical settings, we would use the 99th, 95th, or 90th percentiles. This is importance, since trimming the weights like this induces a degree of potential bias in the estimator. In effect, it is a bias-variance tradeoff being made [@Cole2008].

A final note that is important with these weighted approaches is to consider how to estimate the standard errors. In fact, the model-based standard errors are no longer valid when weighting is used. One must instead use the robust variance estimators, or the bootstrap. For example:

```{r}
coeftest(model_RD_weighted_norm, vcov. = vcovHC)
```

One can then construct CIs in the standard way using the estimated standard error in the output above.

\newpage

# References