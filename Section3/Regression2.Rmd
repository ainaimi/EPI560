---
title: "Regression in Time-Fixed Settings: Part 2"
author: "Ashley I Naimi"
date: "Spring 2022"
urlcolor: blue
bibliography: ref.bib
link-citations: yes
output: 
    bookdown::pdf_book:
      base_format: tint::tintPdf
      toc: true
      number_sections: true
      includes:
        in_header: "../misc/preamble.tex"
      latex_engine: xelatex
    html_document:
      theme: readable
      toc: true
      toc_float: true
      number_sections: true
      css: "../misc/style.css"
---

```{r setup, include=FALSE}
library(knitr)
opts_chunk$set(tidy.opts=list(width.cutoff=40),tidy=TRUE)

packages <- c( "data.table","tidyverse","ggplot2","ggExtra","formatR",
               "gridExtra","skimr","here","RColorBrewer")

for (package in packages) {
  if (!require(package, character.only=T, quietly=T)) {
    install.packages(package, repos='http://lib.stat.cmu.edu/R/CRAN',dependencies=T)
  }
}

for (package in packages) {
  library(package, character.only=T)
}

remotes::install_github("rstudio/fontawesome")

library(fontawesome)

thm <- theme_classic() +
  theme(
    legend.position = "top",
    legend.background = element_rect(fill = "transparent", colour = NA),
    legend.key = element_rect(fill = "transparent", colour = NA)
  )
theme_set(thm)
```

\newpage
\onehalfspacing

\newpage
\onehalfspacing

# Introduction to Regression for Time-to-Event Data

In the last set of notes, we looked at how to use regression to estimate risk differences, risk ratios, and odds ratios, via both conditional adjustment and marginal standardization. However, while these methods can still be used when time-to-event data are available, one may want to pursue a different course. In this set of notes, we are going to look at regression methods for time-to-event outcomes. In particular, we will focus on the Cox proportional hazards regression model, pooled logistic regression, and accelerated failure time models. We will also discuss options when competing risks are present, focusing on how to compute confounder adjusted cumulative subdistribution risks. 

Throughout, we will use the time-to-event dataset we introduced in Section 1 of the course^[Note that I updated the simulated dataset to contain 2000 observations instead of the original 100.]:

```{r, warning=F, message=F}
a <- read_csv(here("data","2022_03_09-Section1_cohort.csv"))

print(a, n=5)

```

Recall, in these data, the outcome had three levels:

```{r, warning=F, message=F}
a %>% count(outcome)
```

\noindent where `0` indicates no event, and event types `1` and `2` represent competing events. Note again that these data are considered "time-to-event" precisely because we have measured the event times measured, represented by the variable `stop`. 

# Cox Proportional Hazards Regression

Perhaps the first method that comes to mind when considering time-to-event analyses is the Cox proportional hazards (PH) model used to compute the hazard ratio. This is arguably one of the most common techniques used to evaluate time-to-event data.

The Cox model is a technique that regresses the *hazard* against a set of covariates. The hazard is a summary outcome measure like the odds, risk, or rate. It is a function of time that captures the instantaneous rate of events at a given time $t$, with a range of $[0, \infty]$. It is often denoted $h(t)$ or $\lambda(t)$, and is defined as:

$$\lambda(t) = \lim_{\Delta t \rightarrow 0+} \frac{P(t \leq T \leq t + \Delta t \mid t < T)}{\Delta t}$$
This function quantifies the probability that the observed event time $T$ is between some index event time $t$ and it's increment $t + \Delta t$, conditional on the set of individuals who are still at risk at the index event time $t$, all as the increment $\Delta t$ for the index event time $t$ approaches zero from the right. 

Clearly, this is one of the first problems with the Cox proportional hazards regression model: **the hazard is a highly unintuitive measure of occurrence.**

More specifically, when interest lies in the effect of some exposure $X$, one can define the hazard as a function of the exposure as well:

$$\lambda(t \mid X=x) = \lim_{\Delta t \rightarrow 0+} \frac{P(t \leq T \leq t + \Delta t \mid t < T, X = x)}{\Delta t}$$
Now, it's quite unconventional to condition the hazard on an exposure this way. Often, one is more likely to write $\lambda(t \mid X=x)$ as $\lambda_x(t)$. But the problem with this is that one may mis-interpret this hazard as a **causal** quantity, defined via potential outcomes.^[Recall, subscript notation is often used to denote potential outcomes.] However, if we wanted to target the causal hazard, we have to be able to make the necessary causal identifiability assumptions.^[Counterfactual consistency, no interference, exchangeabilty, positivity.] Then, one can formulate the hazard in terms of potential outcomes as:

$$\lambda^x(t) = \lim_{\Delta t \rightarrow 0+} \frac{P(t \leq T^x \leq t + \Delta t \mid t < T^x)}{\Delta t}$$
\noindent where $T^x$ represents the outcome that would be observed if the exposure $X$ was set to some value $x$. If $X$ is a binary exposure $X \in [0,1]$,^[Note that we use a binary exposure for simplification. This can easily be generalized to multicategory $X$ or continuous $X$.] then we can then define the associational hazard ratio as $\lambda(t \mid X=1)/\lambda(t \mid X=0)$ or the causal hazard ratio as $\lambda^{x=1}(t)/\lambda^{x=0}(t)$.

Again, for a binary exposure $X \in [0,1]$, the Cox PH model can be motivated as follows: 

\begin{align*}
HR =& \sfrac{\lambda_1(t)}{\lambda_0(t)} \\
\exp(\beta) =& \sfrac{\lambda_1(t)}{\lambda_0(t)} \\
\implies \lambda_1(t) =& \lambda_0(t)\exp(\beta X) \tag{Cox Model}
\end{align*}

:::{.rmdnote data-latex="{tip}"}

__Concept Question__:

|               Should the additive portion of the Cox model contain an intercept? Why or why not?

:::

In effect, the Cox model defines the hazard for those with $X=1$ as a baseline hazard $\lambda_0(t)$ multiplied by some exposure contribution $\exp(\beta X)$. In other words, the hazard for those with $X=1$ is a multiple of the hazard for those with $X=0$. 

```{r, warning=F, message=F, echo=F, fig.show="hide"}
set.seed(123)
time <- runif(200,min=0,max=10)
haz0 <- cos(time) + max(cos(time)) + 2
haz1 <- haz0*exp(.5)
plot_dat <- tibble(time,haz0,haz1)

ggplot(plot_dat) + 
  geom_line(aes(time,log(haz0))) + 
  geom_line(aes(time,log(haz1)),linetype=2) +
  ylab("log-Hazard") + xlab("Time")

ggsave(here("figures","2022_03_08-PH_assumption.pdf"),
       height=8,
       width=8,
       units="cm")
```

Why does this model imply proportional hazards? Note that the second line of the motivating equations above implies that the ratio of two hazards is $\exp(\beta)$, which does not depend on time. In other words, the ratio of two hazards cannot change over time, thus requiring constant proportion across time. An example of this with a fairly unrealistic baselien hazard is shown in Figure \ref{fig:prophaz}. 

```{r prophaz, out.width="4cm", out.height="4cm", fig.margin=TRUE, fig.cap="Illustration of the proportional hazards assumption, where the solid line represents the baseline hazard, and the dashed line represents the increase in the baseline due to some covariate.",echo=F}
knitr::include_graphics(here("figures","2022_03_08-PH_assumption.pdf"))
```

The proportional hazards aspect of the model is often noted as an important limitation of the approach, but nonproportional hazards can easily be accommodated. Proportional hazards would be violated if, for example, the multiplicative effect of a particular variable on the baseline hazard changed over time, leading the hazards to converge or diverge (nonproportionality). 

Several tests exist for evaluating proportional hazards (e.g., Schoenfeld residual plot). One easy approach is to evaluate whether there are important interactions between time and the covariates. If so, leaving these interactions in the model can be one technique to account for the nonproportional nature of the hazards.^[If there is an interaction between time and the exposure of interest, then one has to interpret the association between the exposure and the outcome as a function of this interaction.]

One property of the Cox model that is often raised as a strength of the approach is that it is *semiparametric* in that it does not assume a specific form for the baseline hazard (see Technical Note). 

:::{.rmdnote data-latex="{tip}"}

__Technical Note__:

|               Technically, a parametric model assumes some finite set of parameters $\theta$. Given the parameters, predictions from the model $\hat{y}$ are independent of the observed data $\mathcal{O}$: 

$$P(\hat{y} \mid \theta, \mathcal{O}) = P(\hat{y} \mid \theta).$$
That is, $\theta$ captures everything there is to know about the data. With knowledge of the parameters, we no longer need the data to obtain information about the predictions. 

Because the set of parameters are finite-dimensional, the complexity of the model is bounded, even in (asymptotic, theoretical) settings where we have infinite data.

In contrast, a nonparametric model assumes $\theta$ is *infinite dimensional*. In a nonparametric model, $\theta$ is often considered a function. 

Because the set of parameters are infinite-dimensional, the complexity of the model depends on the complexity of the data. Thus, even in (asymptotic, theoretical) settings where we have infinite data, we can accommodate complexity. 

Finally, a semiparametric model is one that contains both finite-dimensional and infinite-dimensional parameters. The classic example is, in fact, the Cox PH model, but many other semiparametric models exist. Consider a Cox model with two covariates $X_1$ and $X_2$:

$$\lambda(t) = \lambda_0(t)\times \exp(\beta_1X_1 + \beta_2X_2)$$
In this model, the baseline hazard $\lambda_0(t)$ is infinite-dimensional. As a result, it can accommodate any "shape" the data may require. On the other hand, the coefficient terms $\exp(\beta_1X_1 + \beta_2X_2)$ are parametric. As a result, the increase in the baseline hazard due to $X_1$ and $X_2$ are encoded in this model as multiplicative. In other words, this model rules out a potential additive effect of $X_1$ and $X_2$ on the baseline hazard, for instance:

$$\lambda(t) = \lambda_0(t) + \alpha_1X_1 + \alpha_2X_2$$
which would not be the case if the covariate effects were specified nonparametrically.
:::

While its semiparametric feature is indeed a strength, it does not (IMO) overcome the serious limitations inherent in the Cox model, particularly when it comes to quantifying causal effects. 

One important question with the Cox model is how one can quantify model parameters, particularly with a nonparametric baseline hazard? This was one of Sir David Cox's brilliant contributions: the *partial likelihood function* [@Cox1972, @Cox1975]. He showed using the ordered event times that the full likelihood could be re-written without incorporating the baseline hazard. For example, for $K$ ordered event times, the partial likelihood can be written as:

$$\mathcal{L}(\beta) = \prod_{j=1}^K \frac{\lambda_0(T_j)\exp(\beta X_j)}{\sum_{i \in \mathcal{R}(T_j)\lambda_0(T_j)\exp(\beta X_i)}} = \prod_{j=1}^K \frac{\exp(\beta X_j)}{\sum_{i \in \mathcal{R}(T_j)}\exp(\beta X_i)}$$
where the index $i \in \mathcal{R}(T_j)$ refers to all individuals **at risk of the event at time** $T_j$. This is often referred to as the "risk-set". This has important implications for interpreting the hazard ratio, as we will see later. Moreover, because this expression requires *ordering* the event times, the presence of tied event times requires that the likelihood be modified. Many methods exist for handling ties, with Efron's method being the preferred choice in most settings [@HertzPicciotto1997, @Efron1977].

The Cox PH model can easily be fit in R. To do this, we'll evaluate the hazard that the outcome value is 1:

```{r}
library(survival)

a <- a %>% mutate(outcome_one = as.numeric(outcome==1))

cox_model <- coxph(Surv(time=start,time2=stop,event=outcome_one) ~ exposure + confounder, data=a, ties="efron")

summary(cox_model)

```

In this particular case, we estimate an adjusted hazard ratio for the association between the exposure and the time-to-event outcome of `r round(exp(coef(cox_model))[1],2)` with 95% CIs of `r round(summary(cox_model)$conf.int[1,3:4], 2)`.

## The Problem with Cox Regression

Over a decade ago, @Hernan2010 identified important problems with the hazard ratio as a causal estimand. These were:

1) The HR can change over time. That is, the HR early on during follow-up may be different from the HR at the end of follow up. However, researchers usually report a single HR, which represents an average over the entire follow-up period.

2) The HR has a built-in selection bias. This results from the fact that the hazard is a function that conditions on surviving past time $t$ (see also denominator of partial likelihood). So in a study with a long follow-up period, the HR may be biased by the fact that it is dominated by the presence of survivors, suggesting no exposure effect, even when the exposure is harmful early on in follow-up. 

Because of these two problems, it is important to consider other estimands for quantifying exposure effects with time-to-event outcomes.

# Pooled Logistic Regression

Pooled logistic regression is a second approach that can be used to handle time-to-event outcomes [@DAgostino1990]. This approach is an alternative technique that can be used to approximate the hazard ratio, particularly when the outcome is rare. But it can be used for other purposes that we will see (e.g., CDF estimation) that does not require rare outcomes.

Pooled logistic regression can be implemented by fitting a standard logistic regression model for the outcome, conditional on the exposure and relevant confounders. The key condition required to fit a pooled logistic model is the way the data are constructed. So far, our dataset `a` has been analysed in the "single row per observation" format. For the pooled logistic model, the data must be transformed so that each row represents a single time interval. The choice of time interval is important, particularly if one wants to use the pooled logistic model to approximate a hazard ratio. In this case, the interval should be chosen such that the proportion of events in each interval is no more than approximately 10%. 

Let's look at how we can transform the section 1 data into the single row per time-interval. 

```{r}

# first, we create a "stop_rounded" variable, which generates
# an integer count for the time on study. 
a <- a %>% mutate(stop_rounded = ceiling(stop)) # why ceiling?

# this shows how the unrounded time-to-event variable relates
# to the rounded variable
a %>% group_by(stop_rounded) %>% summarize(minStop = min(stop),
                                           medianStop = median(stop),
                                           maxStop = max(stop))

```

Once the integer time interval variable is created, we can start creating the dataset that contains a single row per time interval:
```{r}

# create the `b` dataset, which has multiple rows per person, 
# using the "uncount" function

b <- a %>%
  uncount(stop_rounded) %>% # this row expands the dataset 
                            # based on the count in stop_rounded
  group_by(ID) %>%
  mutate(counter = 1,     # create a counter that can be used to sum by ID
         time_var = cumsum(counter), # sum the counter by ID: 
                                     # this becomes the new "time" variable
         last_id = !duplicated(ID, fromLast = T), # flag the last row for each person
         outcome_one = outcome_one*last_id, # set the new outcome 
                                                # variable to 1 if last 
                                                # AND old outcome is 1
         outcome_two = outcome*last_id) %>% # for competing risks
  ungroup(ID) %>%
  select(ID,time_var,stop,exposure,confounder,outcome,outcome_one,outcome_two)

b %>% filter(ID %in% c(1,3,7))

```

With this dataset, we can now proceed with a pooled logistic regression model analysis. The difference between a "pooled" and standard logistic regression model is that the pooled model includes a flexible function of the time-to-event variable as a conditioning argument, in a dataset arranged in the person-time format. The pooled logistic model is often surreptitiously^[adverb; in a way that attempts to avoid notice or attention.] written as:

$$\logit\{ P(Y = 1 \mid T, X, C) \} = \beta_{0t} + \beta_1 X + \beta_2 C$$
where $\beta_{0t}$ is a time-dependent intercept. In this model, and under the right circumstances (specifically, when the probability of the outcome in any given time interval is less than roughly 10%), one can interpret an estimate of $\beta_1$ as an approximation of the hazard ratio from the Cox PH regression model  [@DAgostino1990]. We can check this in our new dataset easily:

```{r}
b %>% group_by(time_var) %>% summarize(meanOutcome = mean(outcome_one))
```


In practice, this time-dependent intercept can be obtained by including an indicator term for each time-to-event in the model. For example, in R, this can be accomplished using the `factor` function:

```{r, warning=F, message=F}

plr_model <- glm(outcome_one ~ factor(time_var) + exposure + confounder, data=b, family=binomial(link = "logit"))

round(summary(plr_model)$coefficients["exposure",], 2)

```

Let's compare the estimated coefficient from this model to the hazard ratio above. Recall that the Cox model yielded a HR of `r round(exp(coef(cox_model))[1],2)` with 95% CIs of `r round(summary(cox_model)$conf.int[1,3:4], 2)`. The pooled logistic model yields an approximated HR of `r round(exp(summary(plr_model)$coefficients["exposure", "Estimate"]),2)` with 95% confidence intervals of `r round(exp(summary(plr_model)$coefficients["exposure", "Estimate"] - 1.96 * summary(plr_model)$coefficients["exposure", "Std. Error"]),2)` and `r round(exp(summary(plr_model)$coefficients["exposure", "Estimate"] + 1.96 * summary(plr_model)$coefficients["exposure", "Std. Error"]),2)`.

Sometimes, the number of time-intervals created is too large to include in the model as a conditioning statement. For example, in a study with 60 weeks of follow-up, including weekly time-intervals with the factor function in the model would result in a model with at least 60 parameters. To deal with this, researchers will often smooth the time-intervals using polynomials or splines.

```{r, warning=F, message=F}

library(splines)

plr_model_linear <- glm(outcome_one ~ time_var + exposure + confounder, data=b, family=binomial(link = "logit"))

plr_model_squared <- glm(outcome_one ~ time_var + I(time_var^2) + exposure + confounder, data=b, family=binomial(link = "logit"))

plr_model_cubed <- glm(outcome_one ~ time_var + I(time_var^2) + I(time_var^3) + exposure + confounder, data=b, family=binomial(link = "logit"))

plr_model_spline <- glm(outcome_one ~ bs(time_var, df = 4) + exposure + confounder, data=b, family=binomial(link = "logit"))

round(summary(plr_model_linear)$coefficients["exposure", c("Estimate", "Std. Error")],2)

round(summary(plr_model_squared)$coefficients["exposure", c("Estimate", "Std. Error")],2)

round(summary(plr_model_cubed)$coefficients["exposure", c("Estimate", "Std. Error")],2)

round(summary(plr_model_spline)$coefficients["exposure", c("Estimate", "Std. Error")],2)

```

In these data, the functional form of the time variable does not matter.^[This is to be expected, because we simulated these time-to-events from an exponential distribution. This means that the simple linear term in `plr_model_linear` is actually compatible with the data generating mechanism. Additional flexibility is not required.] This is a common usage of the pooled logistic regression model, particularly when (as we will see) the exposure and confounders are **time-varying.** One way to read the results of the pooled logistic regression output is to read the coefficient for the exposure in the model. As noted, when the outcome is rare within each unique time-interval specified in the model (or across the range of the continuous function for time in the model), this coefficient can be interpreted as an approximation to the hazard ratio.

This is a second procedure that can be used to quantify a hazard ratio. However, one still has all the same problems with interpreting the hazard ratio. Fortunately, we can do a bit more with this model than we can with the Cox model. In particular, we can generate time-specific predicted probabilities for the outcome for each individual in the sample under exposed and unexposed states^[Technically, we *can* do this with the Cox model. We just need to use an estimator for the baseline hazard (which is not quantified in the Cox model), such as the Breslow estimator.]:

```{r}

plr_model_spline <- glm(outcome_one ~ bs(time_var, df = 4) + exposure + confounder, data=b, family=binomial(link = "logit"))

summary(plr_model_spline)

# create three datasets, one that predicts under natural conditions
# and two that predict under exposure = [0, 1]
mu <- tibble(b,mu=predict(plr_model_spline,newdata=b,type="response"))
mu1 <- tibble(b,mu1=predict(plr_model_spline,newdata=transform(b,exposure=1),type="response"))
mu0 <- tibble(b,mu0=predict(plr_model_spline,newdata=transform(b,exposure=0),type="response"))

# average the predictions for each individual stratified by time
mu <- mu %>% group_by(time_var) %>% summarize(mean_mu=mean(mu))
mu1 <- mu1 %>% group_by(time_var) %>% summarize(mean_mu1=mean(mu1))
mu0 <- mu0 %>% group_by(time_var) %>% summarize(mean_mu0=mean(mu0))

# cumulatively sum the predictions over time to estimate cumulative risk
mu <- mu %>% mutate(cum_risk = cumsum(mean_mu))
mu1 <- mu1 %>% mutate(cum_risk = cumsum(mean_mu1))
mu0 <- mu0 %>% mutate(cum_risk = cumsum(mean_mu0))

mu
mu1
mu0

```

In effect, this procedure is marginal standardization. Except, rather than only estimating the risk at the end of follow-up under exposed and unexposed states, we are quantifying the cumulative risk function over follow-up time. Note also, this cumulative risk function is marginally adjusted for the confounder. Thus, here we have a way to compute marginally standardized risk functions using a pooled logistic model. This approach, however, will only work with a single outcome of interest. 

# Multinomial Logistic Regression: Competing Risks

In the situation where competing events are present, the pooled logistic regression approach can be extended by replacing the binomial distribution with the multinomial distribution. The multinomial distribution is a generalization of the binomial distribution to scenarios where the event of interest can take on more than one value. One can define a simple multinomial logistic regression model as:

$$\logit[P(Y=k \mid X, C)] = \beta_{0k} + \beta_{1k} X + \beta_{2k} C$$
for $k = 1, \ldots, K$. In this simple model for a binary $X$ variable, $\exp(\beta_1)$ can be interpreted as the following odds ratio:

$$\sfrac{\frac{P(Y = k \mid X = 1, C)}{P(Y = k^{\prime} \mid X = 1, C)}}{\frac{P(Y = k \mid X = 0, C)}{P(Y = k^{\prime} \mid X = 0, C)}}$$

where $Y = k^{\prime}$ refers to some referent outcome level. 

For example, in our example data, the outcome takes on three levels: $Y \in [0, 1, 2]$. Thus, if we take $Y = 0$ as our referent level, we obtain two odds ratios from this model:

$$\sfrac{\frac{P(Y = 2 \mid X = 1, C)}{P(Y = 0 \mid X = 1, C)}}{\frac{P(Y = 2 \mid X = 0, C)}{P(Y = 0 \mid X = 0, C)}}$$

and

$$\sfrac{\frac{P(Y = 1 \mid X = 1, C)}{P(Y = 0 \mid X = 1, C)}}{\frac{P(Y = 1 \mid X = 0, C)}{P(Y = 0 \mid X = 0, C)}}$$
This multinomial model can also be adapted to the time-to-event setting in the same way we adapted the logistic regression model. In effect, we can fit a pooled multinomial model as:

$$\logit[P(Y=k \mid X, C)] = \beta_{0tk} + \beta_{1k} X + \beta_{2k} C$$
which effectively makes each outcome-specific intercept a function of time.

In R, their are several packages that can be used to fit a multinomial model, including the `multinom` function in the `nnet`, and the `vglm` function in the `VGAM` package. We'll use the latter here. 

Let's first look at the data we'll be fitting:

```{r}

b %>% filter(ID %in% c(1,3,7))

```

Note that the `outcome_two` variable is indeed a three-level variable:

```{r}

b %>% count(outcome_two)

```

To implement the pooled multinomial model for competing risks, we can use the `vglm` function. To allow for flexibility along the time dimension, we'll use splines:

```{r}

library(VGAM)

pmr1 <- vglm(outcome_two ~ time_var + exposure + confounder, 
             data=b, family=multinomial(refLevel = 1)) ## note the need for refLevel

summary(pmr1)@coef3[c("exposure:1","exposure:2"),]

```

However, once again, the coefficients in this model will be challenging to interpret. We can implement the same procedure with the binomial model to obtain adjusted sub-distribution risk curves. To do this, it's useful to see what we obtain when we apply the predict function to an object resulting from the fit of the `vglm` function:

```{r}
head(predict(pmr1, newdata=b, type="response"))
```

The output from the predict function with a multinomial model (fitted with `vglm`) is a $K$ column dataset, where $K$ is the number of outcome levels. We thus have to adjust our code to ensure we select the correct probabilites when creating standardized risk curves:

```{r}

pmr1 <- vglm(outcome_two ~ scale(time_var) + exposure + confounder, 
             data=b, family=multinomial(refLevel = 1))

# create three datasets, one that predicts under natural conditions
# and two that predict under exposure = [0, 1]
## FOR OUTCOME = 1
mu_1 <- tibble(b,mu_1=predict(pmr1,newdata=b,type="response")[,2])
mu_11 <- tibble(b,mu_11=predict(pmr1,newdata=transform(b,exposure=1),type="response")[,2])
mu_10 <- tibble(b,mu_10=predict(pmr1,newdata=transform(b,exposure=0),type="response")[,2])
## FOR OUTCOME = 2
mu_2 <- tibble(b,mu_2=predict(pmr1,newdata=b,type="response")[,3])
mu_21 <- tibble(b,mu_21=predict(pmr1,newdata=transform(b,exposure=1),type="response")[,3])
mu_20 <- tibble(b,mu_20=predict(pmr1,newdata=transform(b,exposure=0),type="response")[,3])

# average the predictions for each individual stratified by time
## FOR OUTCOME = 1
mu_1 <- mu_1 %>% group_by(time_var) %>% summarize(mean_mu_1=mean(mu_1))
mu_11 <- mu_11 %>% group_by(time_var) %>% summarize(mean_mu_11=mean(mu_11))
mu_10 <- mu_10 %>% group_by(time_var) %>% summarize(mean_mu_10=mean(mu_10))
## FOR OUTCOME = 2
mu_2 <- mu_2 %>% group_by(time_var) %>% summarize(mean_mu_2=mean(mu_2))
mu_21 <- mu_21 %>% group_by(time_var) %>% summarize(mean_mu_21=mean(mu_21))
mu_20 <- mu_20 %>% group_by(time_var) %>% summarize(mean_mu_20=mean(mu_20))

# cumulatively sum the predictions over time to estimate cumulative risk
## FOR OUTCOME = 1
mu_1 <- mu_1 %>% mutate(cum_risk = cumsum(mean_mu_1))
mu_11 <- mu_11 %>% mutate(cum_risk = cumsum(mean_mu_11))
mu_10 <- mu_10 %>% mutate(cum_risk = cumsum(mean_mu_10))
## FOR OUTCOME = 2
mu_2 <- mu_2 %>% mutate(cum_risk = cumsum(mean_mu_2))
mu_21 <- mu_21 %>% mutate(cum_risk = cumsum(mean_mu_21))
mu_20 <- mu_20 %>% mutate(cum_risk = cumsum(mean_mu_20))

mu_1
mu_11
mu_10

mu_2
mu_21
mu_20

```





# References